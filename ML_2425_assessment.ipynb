{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1308ea1-b9a2-4f7a-b000-f41d0b38aff2",
   "metadata": {},
   "source": [
    "# Machine Learning (CMP3751M/CMP9772M) - Assessment 02\n",
    "\n",
    "Through the following notebook, you will be analysing a dataset and fitting a classification model to this dataset.\n",
    "\n",
    "The assessment is structured as follows:\n",
    "- [Dataset description](#Dataset-description)\n",
    "- [Loading the dataset](#Loading-the-dataset)\n",
    "- [Simple classification model](#Simple-classification-model)\n",
    "    - [Creating a training and testing set](#Creating-a-training-and-testing-set)\n",
    "    - [Training a classifier](#Training-a-classifier)\n",
    "- [Improved evaluation strategy](#Improved-evaluation-strategy)\n",
    "- [Different models and parameter search](#Different-models-and-parameter-search)\n",
    "- [Ensembles](#Ensembles)\n",
    "- [Final model evaluation](#Final-model-evaluation)\n",
    "- [References](#References)\n",
    "\n",
    "**Notes:**\n",
    "- Any discussion not supported by your implementation will not be awarded marks.\n",
    "- **Do not modify** any code provided as a **TESTING CELL**.\n",
    "- Make sure to **fix all the random seeds** in any parts of your solution, so it can be reproduced exactly.\n",
    "- The notebook, as provided, runs without errors (without solving the assessment). Make sure that the solution, or the partial solution, you hand in, also **runs without errors** on the data provided. If you have a partial solution causing errors which you would like to show, please include it as a comment.\n",
    "- Take care to include references to any external sources used. Check the [References](#References) section, the below cell, and the exambles through the assessment text for examples of how to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f54719c9-3bc8-47c1-b620-831b73083ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to reference your sources! Check the bottom of the file, and examples used in the text of the assessment,\n",
    "# for including references to papers and software in your textual answers\n",
    "\n",
    "# Also add a reference in your solution cell before defining a class/function/method, eg.:\n",
    "\n",
    "# This code is a modified and extended version of [3]\n",
    "# OR\n",
    "# This code is a modified and extended version of https://stackoverflow.com/q/522563/884412\n",
    "##############\n",
    "## THE CODE ##\n",
    "##############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97af12af-dd93-44ba-a161-dbab8f2017c0",
   "metadata": {},
   "source": [
    "## Dataset description\n",
    "\n",
    "The the assessment will be done on the dataset containing only numerical features describing the size and shape features of different varieties of dry beans [1]. (The dataset for this assessment has been adapted from the full dataset which can be found [here](https://www.muratkoklu.com/datasets/) [2]), shared in the public domain by the author).\n",
    "\n",
    "Each sample describes the measurements of a bean of a single variety, and consists of following 16 features:\n",
    "\n",
    "| Feature Name      | Value | Description |\n",
    "| :---------------- | :----- | ----------- |\n",
    "| `Area`       | `float` | Area of the bean in pixels. |\n",
    "| `Perimeter` | `float` | Bean circumference is defined as the length of its border. |\n",
    "| `MajorAxisLength` | `float` | The distance between the ends of the longest line that can be drawn from a bean. |\n",
    "| `MinorAxisLength` | `float` | The longest line that can be drawn from the bean while standing perpendicular to the main axis. |\n",
    "| `AspectRatio` | `float` | The ratio between the major and minor axis length. |\n",
    "| `Eccentricity` | `float` | Eccentricity of the ellipse having the same moments as the region. |\n",
    "| `ConvexArea` | `int` | Number of pixels in the smallest convex polygon that can contain the area of a bean seed. |\n",
    "| `EquivDiameter` | `float` | The diameter of a circle having the same area as a bean seed area. |\n",
    "| `Extent` | `float` | The ratio of the pixels in the bounding box to the bean area. |\n",
    "| `Solidity` | `float` | Also known as convexity. The ratio of the pixels in the convex shell to those found in beans. |\n",
    "| `Roundness`| `float` | Measures the roundness of an object. |\n",
    "| `Compactness` | `float` | An alternative measure of object roundness. |\n",
    "| `ShapeFactor1` | `float` | Shape features according to [4] |\n",
    "| `ShapeFactor2` | `float` | Shape features according to [4] |\n",
    "| `ShapeFactor3` | `float` | Shape features according to [4] |\n",
    "| `ShapeFactor4` | `float` | Shape features according to [4] |\n",
    "\n",
    "\n",
    "\n",
    "The goal for the assessment is to predict the variety of bean, listed in the last column, which provides a classification for each sample:\n",
    "\n",
    "| Class      | Value | Description |\n",
    "| :---------------- | :----- | ----------- |\n",
    "| `Class`  | `string`: class designation | The variety of dry bean. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e3ee2b-8c56-49f4-a5da-bbc7330283cb",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "The dataset is given in _beans.csv_ file provided on Blackboard. **Load the dataset into two [`numpy.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html)s.**: \n",
    "- The variable `X` should be a 2D [`numpy.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html) containing all the samples and their features from the dataset, one sample per row. \n",
    "- The variable `y` should be a 1D [`numpy.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html) containing the ground truth (class) as given in the `'Class'` field of the _.csv_ file.\n",
    "- _Note_: The class in the `'Class'` column is given as a string. Make sure you encode the class as an integer number in your ground truth `y`.\n",
    "- _Note_: You should make sure that your code for loading the dataset is guided by the information about the dataset, and the dataset description you provide as your answer.\n",
    "\n",
    "**Describe the dataset**. Provide a basic description of the dataset. How many samples are there in the dataset? How many distinct classes? What types of features describe the samples in the dataset? Are there any missing values in the dataset? (Make sure these are properly handled). \n",
    "- _Note_: Make sure all your answers are supported by your implementation. Answers not supported by your implementation will not score any marks.\n",
    "\n",
    "Provide your code to _load the dataset_ and the code that will allow you to _describe the dataset_ in the **SOLUTION CELL**. Provide your description of the dataset in the **ANSWER CELL**. A correct solution should result in no errors when running the **TESTING CELL** provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc2b74-5ce4-47ba-815a-138e893c599c",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ae8e88a2-d3c4-4b7a-b8b9-b679fa6f6b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>AspectRatio</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>EquivDiameter</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>roundness</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>ShapeFactor1</th>\n",
       "      <th>ShapeFactor2</th>\n",
       "      <th>ShapeFactor3</th>\n",
       "      <th>ShapeFactor4</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68035</td>\n",
       "      <td>1022.207</td>\n",
       "      <td>355.899595</td>\n",
       "      <td>244.028109</td>\n",
       "      <td>1.458437</td>\n",
       "      <td>0.727917</td>\n",
       "      <td>69243</td>\n",
       "      <td>294.321002</td>\n",
       "      <td>0.797017</td>\n",
       "      <td>0.982554</td>\n",
       "      <td>0.818210</td>\n",
       "      <td>0.826978</td>\n",
       "      <td>0.005231</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.683892</td>\n",
       "      <td>0.997413</td>\n",
       "      <td>BARBUNYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66871</td>\n",
       "      <td>990.128</td>\n",
       "      <td>372.968458</td>\n",
       "      <td>229.417890</td>\n",
       "      <td>1.625717</td>\n",
       "      <td>0.788439</td>\n",
       "      <td>67765</td>\n",
       "      <td>291.792395</td>\n",
       "      <td>0.801868</td>\n",
       "      <td>0.986807</td>\n",
       "      <td>0.857166</td>\n",
       "      <td>0.782351</td>\n",
       "      <td>0.005577</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.612074</td>\n",
       "      <td>0.995058</td>\n",
       "      <td>BARBUNYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89235</td>\n",
       "      <td>1197.862</td>\n",
       "      <td>395.687961</td>\n",
       "      <td>288.807741</td>\n",
       "      <td>1.370074</td>\n",
       "      <td>0.683567</td>\n",
       "      <td>90764</td>\n",
       "      <td>337.071996</td>\n",
       "      <td>0.769082</td>\n",
       "      <td>0.983154</td>\n",
       "      <td>0.781505</td>\n",
       "      <td>0.851863</td>\n",
       "      <td>0.004434</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.725671</td>\n",
       "      <td>0.994223</td>\n",
       "      <td>BARBUNYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60408</td>\n",
       "      <td>999.737</td>\n",
       "      <td>369.487535</td>\n",
       "      <td>208.936425</td>\n",
       "      <td>1.768421</td>\n",
       "      <td>0.824765</td>\n",
       "      <td>61404</td>\n",
       "      <td>277.333472</td>\n",
       "      <td>0.742478</td>\n",
       "      <td>0.983780</td>\n",
       "      <td>0.759509</td>\n",
       "      <td>0.750590</td>\n",
       "      <td>0.006117</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.563385</td>\n",
       "      <td>0.996301</td>\n",
       "      <td>BARBUNYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70344</td>\n",
       "      <td>1037.985</td>\n",
       "      <td>378.651095</td>\n",
       "      <td>237.909773</td>\n",
       "      <td>1.591574</td>\n",
       "      <td>0.777964</td>\n",
       "      <td>71521</td>\n",
       "      <td>299.273725</td>\n",
       "      <td>0.821354</td>\n",
       "      <td>0.983543</td>\n",
       "      <td>0.820455</td>\n",
       "      <td>0.790368</td>\n",
       "      <td>0.005383</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.624682</td>\n",
       "      <td>0.994227</td>\n",
       "      <td>BARBUNYA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRatio  \\\n",
       "0  68035   1022.207       355.899595       244.028109     1.458437   \n",
       "1  66871    990.128       372.968458       229.417890     1.625717   \n",
       "2  89235   1197.862       395.687961       288.807741     1.370074   \n",
       "3  60408    999.737       369.487535       208.936425     1.768421   \n",
       "4  70344   1037.985       378.651095       237.909773     1.591574   \n",
       "\n",
       "   Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  roundness  \\\n",
       "0      0.727917       69243     294.321002  0.797017  0.982554   0.818210   \n",
       "1      0.788439       67765     291.792395  0.801868  0.986807   0.857166   \n",
       "2      0.683567       90764     337.071996  0.769082  0.983154   0.781505   \n",
       "3      0.824765       61404     277.333472  0.742478  0.983780   0.759509   \n",
       "4      0.777964       71521     299.273725  0.821354  0.983543   0.820455   \n",
       "\n",
       "   Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  \\\n",
       "0     0.826978      0.005231      0.001509      0.683892      0.997413   \n",
       "1     0.782351      0.005577      0.001289      0.612074      0.995058   \n",
       "2     0.851863      0.004434      0.001440      0.725671      0.994223   \n",
       "3     0.750590      0.006117      0.001198      0.563385      0.996301   \n",
       "4     0.790368      0.005383      0.001296      0.624682      0.994227   \n",
       "\n",
       "      Class  \n",
       "0  BARBUNYA  \n",
       "1  BARBUNYA  \n",
       "2  BARBUNYA  \n",
       "3  BARBUNYA  \n",
       "4  BARBUNYA  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.read_csv('beans.csv')\n",
    "\n",
    "\n",
    "data = data.bfill()   #Handling missing data https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.bfill.html\n",
    "\n",
    "#Seperating features and class as required\n",
    "X = data.drop(columns = ['Class']).values #Store all features except class in x, .values converts the data to a numpy array\n",
    "y = data['Class'].values #Store Class Labels .values converts the data to a numpy array\n",
    "\n",
    "#Encode class labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)  #converts string to integer\n",
    "\n",
    "data.head() #Output the head to see a preview of the dataset using the .head() method. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae52cc-9f39-4009-b684-cafb1cd35361",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d3ed4f9-8d94-41f8-8f12-b18919fbe884",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(X.shape) == 2)\n",
    "assert(len(y.shape) == 1)\n",
    "assert(X.shape[0] == y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60db068-206c-4471-8b54-780a653cc315",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e4e42d-cc07-4982-91fb-97c7b2f282a1",
   "metadata": {},
   "source": [
    "1. The dataset contains 2900 samples.\n",
    "2. The dataset contains 16 Numerical Features stored in X, these describe the geometric properties. The dataset also contains 1 Categorical feature Class stored in y.\n",
    "3. There are 7 Unique Classes in the dataset. ( BARBUNYA , BOMBAY , CALI , DERMASON , HOROZ , SEKER, SIRA )\n",
    "4. Missing Values were handled using the backward fill function ( .bfill() ) which copies the next downward target in the column to fill the missing value. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.bfill.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0117c-fdca-4d5a-9139-c0addbeb94a2",
   "metadata": {},
   "source": [
    "## Simple classification model\n",
    "\n",
    "To get the feel for the dataset, the first step will be to build train a simple classification model for this dataset. Do this in two steps detailed below:\n",
    "1. Set aside some data for training and for testing.\n",
    "2. Train a simple classifier on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac10b8-eac0-4b64-a82e-40781846c8d1",
   "metadata": {},
   "source": [
    "### Creating a training and testing set\n",
    "\n",
    "**Set aside 20\\% of the data for testing, and use the remaining 80\\% to train your model.** Make sure to fix any random seeds if you use any functions or methods relying on those, so your experiments are _fully repeatable_. Initialise the following variables:\n",
    "- `X_train` should contain the features corresponding to your training data.\n",
    "- `y_train` should contain the ground truth of your training data.\n",
    "- `X_test` should contain the features corresponding to your testing data.\n",
    "- `y_train` should contain the ground truth associated to your testing data.\n",
    "\n",
    "_Note:_ No additional marks will be rewarded for implementing an advanced data splitting strategy on this task. The purpose of this task is to start working with the dataset by applying a simple approach; you will have the chance to implement more complex evaluation pipelines in a later task.\n",
    "\n",
    "Provide your implementation in the **SOLUTION CELL (a)** below. A correct solution should result in no errors when running the **TESTING CELL** provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02bc50e-8318-428d-b29b-220b4dadb283",
   "metadata": {},
   "source": [
    "### Training a classifier\n",
    "\n",
    "**Train a simple classifier,** (of your choosing) **with fixed parameters** on the dataset, and **calculate accuracy on the test set**.\n",
    "- Define a function `model_accuracy(y_test, y_pred)` to compare the ground truth given in `y_test` to predictions given in `y_pred` and calculate accuracy.\n",
    "- **Store the model** in the variable named `model`. For the model, you may chose any classifier with which you are familiar (e.g. K Nearest Neighbours), or implement your own classifier. Make sure you **train your model** using the _training data_ only (`X_train`, `y_train`).\n",
    "- Use the model to **predict the classes of the data** in the testing set (`X_test`), and calculate the accuracy by comparing the predictions with the ground truth for the testing set (`y_test`). **Store the predictions** in a variable called `y_pred`.\n",
    "\n",
    "_Note:_ Do not implement an advanced strategy to chose the parameters of your classifier here, as that will be a topic of a latter question.\n",
    "\n",
    "_Note:_ If you implement your own classifier, make sure you implement it as a _class_ following the _sklearn_ standard for classifiers (i.e. make sure it implements the `fit(X, y)` method to train the model, and `predict(X)` method to use the trained model to predict the classes of provided samples.\n",
    "\n",
    "\n",
    "**Discuss the advantages and shortcomings** of the evaluation strategy implemented through this task. Discuss both the data split used for evaluation and the choice of metric. Taking into account the information you know about the dataset, what kind of accuracy scores can you expect on this dataset from a good and bad performing model? Based on the information you have so far, comment on the performance of the model you have trained on the provided dataset.\n",
    "\n",
    "Provide your implementation in the **SOLUTION CELL (b)** below. The **TESTING CELL** below should run without errors and will print the prediction of your model for the first sample in the test set, and the accuracy as calculated by your `model_accuracy` function. Provide your discussion in the **ANSWER CELL** below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d0156-553b-4c89-a594-fa1c88e592b4",
   "metadata": {},
   "source": [
    "**SOLUTION CELL (a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c823edbd-0d8b-4238-9ae7-db683d651b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Split the data to 80% training data 20% testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d046238d-602f-4ed1-8f41-cd71fdbb853d",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31fc35ed-610c-47bc-acce-0060f0f72034",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(X_train.shape[0] == y_train.shape[0])\n",
    "assert(X_test.shape[0] == y_test.shape[0])\n",
    "assert(X_train.shape[1] == X_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af37dd-393c-4278-9972-c10677ad6189",
   "metadata": {},
   "source": [
    "**SOLUTION CELL (b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af0e7c2a-5d95-4071-ae09-68c14e00bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def model_accuracy(y_test, y_pred):\n",
    "    #Calculates the accuracy of the model using the accuracy_score function from sklearn metrics. https://www.educative.io/answers/what-is-the-accuracyscore-function-in-sklearn\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors = 3) # Use 3 neighbours for the knn model\n",
    "\n",
    "model.fit(X_train, y_train) #Use the training data to train the model\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4e3e6-be93-4fd5-9611-ec06c2a4610a",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b1d52b9-7543-4d59-90d6-43ad5cf08229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "0.6396551724137931\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_test[0].reshape(1,-1)))\n",
    "print(model_accuracy(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae302f-0899-481e-8901-de9efe493da1",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82bd068-84e1-4014-8696-02e3abfe8455",
   "metadata": {},
   "source": [
    "##### Evaluation Strategy\n",
    "- Dataset was split 80% Training and 20% Testing. THis ensures the model is evaluated on unseen data, provding a quick measure of the model's performance\n",
    "- Evaluation metric used was accuracy, thios measures proprotion of correctly classified samples / the total number of samples. Given the dataset is balanaced from the information gathered so far, this this was deemeded an appropriate metric. https://www.educative.io/answers/what-is-the-accuracyscore-function-in-sklearn\n",
    "\n",
    "##### Model Performance\n",
    "- Trained classifier predicted the first sample in the test (y_pred for X_test[0]) set as [0] which corressponds to the first encoded class. (BARBUNYA)\n",
    "- Overall accruacy was 63.97% (0.639655) indicating moderate performance of the model.\n",
    "- A good model would be expected achieve 90% + given the deatures in the dataset provide meaningful information and meta data about the classes.\n",
    "- A poor model woud achieve accuracy through random guessing which would be significantly lower than 63.97%\n",
    "- Due to this the current model's performance can be deemed as moderate and can be optimized using things such as hyperparameter tuning or feature selection.\n",
    "- (https://www.datacamp.com/tutorial/k-nearest-neighbor-classification-scikit-learn)\n",
    "\n",
    "##### Shortcomings\n",
    "- With a single train-test split, the performance may depend on how the data was split, leading to unreliable results and variablity in results. https://builtin.com/data-science/train-test-split , https://towardsdatascience.com/3-tips-for-working-with-imbalanced-datasets-a765a0f3a0d0\n",
    "- While accuracy provides an over all metric on the model's performance, it does not provide any insight on how the model performed for each class. In datasets with imbalanced classes metrics such as precision recall and F1-score are important to provide these insights. https://keylabs.ai/blog/understanding-the-f1-score-and-auc-roc-curve/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa832abe-5688-43e2-8d1f-b655b7d4b142",
   "metadata": {},
   "source": [
    "## Improved evaluation strategy\n",
    "\n",
    "After discussing the shortcomings of the simple evaluation strategy used in the previous task, you now have a chance to **propose a better evaluation strategy.** Make sure your chosen strategy **uses all the samples in the dataset** to report the result.\n",
    "- **Implement a function** `evaluate_model(model, X, y)` to implement your proposed evaluation strategy. The function should evaluate the model given in `model` on the dataset given by `X` with ground truth given by `y`. Note that the function should be passed the _whole of the dataset_ (see **TESTING CELL** below) and should take care of any data splitting internally.\n",
    "- If desired, you may add additional arguments to this function, as long as they have default values and the function runs correctly when called using those default values.\n",
    "- The function should return no values, but instead print the results of the evaluation in a human-readable format.\n",
    "- Include at least one summative metric (providing a single number, e.g. accuracy) and per-class metric (e.g. precision) calculated for every class. You are encouraged to select more than one metric of each type.\n",
    "\n",
    "This function will be used to provide a better evaluation of the simple model with fixed parameters used in the previous task.\n",
    "\n",
    "**Discuss your chosen evaluation strategy**, including both the data split and the evaluation metrics. Which data splitting strategy did you chose and why? Which metrics did you chose, and why? Briefly explain the chosen data splitting strategy. What additional information can your additional metrics provide beyond accuracy?\n",
    "\n",
    "Provide your implementation of this function in the **SOLUTION CELL**. You may also include any additional evaluation calls you want to include in this code cell. The **TESTING CELL** will perform a basic evaluation of your `model` using the `evaluate_model` function implemented for this task. Provide your discussion in the **ANSWER CELL** below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e4c6a-7818-4b68-a7db-14e73dae16b8",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd9c1591-9cfa-4831-9f56-d42557f31897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def evaluate_model(model, X, y): #n_folds defines the number of folds used for cross-validation\n",
    "    skf = StratifiedKFold(n_splits = 5, shuffle= True, random_state=42)\n",
    "    fold = 1\n",
    "    accuracies = [] #Empty array to hold accuracy to calculate the Mean Accuracy and Standard Diviation of Accuracy\n",
    "    print('Cross Validation Results:')\n",
    "    print('=' * 50) # Creates after the title for a cleaner output layout and more interperetable\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        #Split data for current fold index for iteration purposes\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        #Train Model using current fold split\n",
    "        model.fit(X_train, y_train)\n",
    "        #Make Prediction\n",
    "        y_pred = model.predict(X_test)\n",
    "        #Calculate Accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy) #Append the current fold's accuracy to the accuracies array\n",
    "\n",
    "        #Printing out the acccuracy and metrics per class\n",
    "        print(f'Fold {fold} Accuracy: {accuracy:.4f}') #Outputs the fold number and accuracy of that fold\n",
    "        print(f'Metric Per Class:')\n",
    "        #Output the classification report for current fold, showing the precision / recall / f1-score / support for each class label\n",
    "        print(classification_report(y_test, y_pred, target_names=label_encoder.classes_)) #https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.classification_report.html\n",
    "        print('-' * 50) #Creates a line at the bottom of the current classifcation report to seperate it from the next fold\n",
    "        fold += 1 #Iterate thropugh the folds in the for loop to move to the next train and test index\n",
    "\n",
    "     # Summarize results across folds\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies):.4f}\")\n",
    "    print(f\"Standard Deviation: {np.std(accuracies):.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa6d8f2-fae8-4f49-839c-3c145b4573c3",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73a5b7ca-e08d-4bf0-8b0b-2aeb4ee4982a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Results:\n",
      "==================================================\n",
      "Fold 1 Accuracy: 0.6914\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.43      0.46      0.44        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.59      0.57      0.58        60\n",
      "    DERMASON       0.79      0.92      0.85       200\n",
      "       HOROZ       0.69      0.62      0.65        60\n",
      "       SEKER       0.62      0.36      0.46        80\n",
      "        SIRA       0.64      0.67      0.66       110\n",
      "\n",
      "    accuracy                           0.69       580\n",
      "   macro avg       0.68      0.66      0.66       580\n",
      "weighted avg       0.68      0.69      0.68       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 2 Accuracy: 0.6638\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.46      0.46      0.46        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.65      0.62      0.63        60\n",
      "    DERMASON       0.76      0.86      0.81       200\n",
      "       HOROZ       0.57      0.45      0.50        60\n",
      "       SEKER       0.68      0.34      0.45        80\n",
      "        SIRA       0.56      0.71      0.63       110\n",
      "\n",
      "    accuracy                           0.66       580\n",
      "   macro avg       0.67      0.63      0.64       580\n",
      "weighted avg       0.66      0.66      0.65       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 3 Accuracy: 0.6828\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.36      0.36      0.36        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.61      0.58      0.60        60\n",
      "    DERMASON       0.79      0.90      0.84       200\n",
      "       HOROZ       0.51      0.60      0.55        60\n",
      "       SEKER       0.73      0.38      0.50        80\n",
      "        SIRA       0.68      0.70      0.69       110\n",
      "\n",
      "    accuracy                           0.68       580\n",
      "   macro avg       0.67      0.65      0.65       580\n",
      "weighted avg       0.68      0.68      0.67       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 4 Accuracy: 0.6655\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.43      0.60      0.50        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.68      0.53      0.60        60\n",
      "    DERMASON       0.79      0.89      0.84       200\n",
      "       HOROZ       0.52      0.50      0.51        60\n",
      "       SEKER       0.55      0.38      0.44        80\n",
      "        SIRA       0.62      0.60      0.61       110\n",
      "\n",
      "    accuracy                           0.67       580\n",
      "   macro avg       0.66      0.64      0.64       580\n",
      "weighted avg       0.66      0.67      0.66       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 5 Accuracy: 0.6724\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.43      0.50      0.46        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.60      0.55      0.57        60\n",
      "    DERMASON       0.80      0.90      0.85       200\n",
      "       HOROZ       0.58      0.48      0.53        60\n",
      "       SEKER       0.60      0.47      0.53        80\n",
      "        SIRA       0.59      0.59      0.59       110\n",
      "\n",
      "    accuracy                           0.67       580\n",
      "   macro avg       0.66      0.64      0.65       580\n",
      "weighted avg       0.67      0.67      0.67       580\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Mean Accuracy: 0.6752\n",
      "Standard Deviation: 0.0105\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46babc-b9bb-4825-9a24-2f4ef812d493",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6281e9-3b43-41fd-9c67-fd2caf83a071",
   "metadata": {},
   "source": [
    "##### Chosen Evaluation Strategy\n",
    "- Stratified K-Fold Cross Validation was used as it ensures all samples are used for training and testing. Cross Validation also maintains class proportions in each fold which is important for datasets with imbalanced classes. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "- Using multiple splits, in this case 5 splits reduces the variance in evaluation and helps us obtain reliable estimates on the model's generalization performance in comparison to using a single train test split. https://towardsdatascience.com/3-tips-for-working-with-imbalanced-datasets-a765a0f3a0d0\n",
    "##### Chosen Metrics\n",
    "- Accuracy measures the overall correctness or accuracy of the predictions made by the model. This is a standard metric when evaluating a model's performance.\n",
    "- Precision Recall and F1 - Score were chosen as they give more in depth insight on how the model handles each indivual class.\n",
    "    - Precision provides an insight on the relevance of the predictions made.\n",
    "    - Recall measures the ability to retrieve all relevant instances in the class.\n",
    "    - F1-Score is used to balance precision and recall offering a balanced measure of a classifier's performance. It is very useful for imbalanced datasets where FP and FN play a significant role. https://keylabs.ai/blog/understanding-the-f1-score-and-auc-roc-curve/\n",
    "\n",
    "##### Advantages of New Evaluation Strategy\n",
    "- Cross Validation ensures a robust evaluation as it uses all data for training and testing, reducing the biases due to the 5 splits it creates.\n",
    "- The per class metrics provided by my new evaluation strategy provide a deeper insight on how the model performs for each class. This can help identify classes that are poorly handled by the model. After reviewing the evaluation it is evident that our model's performance is poor when it comes to the classes BARBUNYA and SEKER as the f1 scores for these are the lowest across all 5 folds.\n",
    "- The mean and standard diviation of the acurracy across all folds of the evaluation model provides an estimate of the model's stability and consistency as well as over all performance.\n",
    "- This new evaluation strategy highlights insights using Per Class Metrics which can help determine that BOMBAY class shows perfect precision recall and f1-score of 1.00 for all the folds. This helps the arrival at the conclusion that BOMBAY is a rare class and must have unique features such as a high area which isolate it from the rest of the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ce4cf3-0f68-4884-bf55-e76eed093144",
   "metadata": {},
   "source": [
    "## Different models and parameter search\n",
    "\n",
    "Now that you have a [better evaluation strategy](#Improved-evaluation-strategy) implemented, it is time to try out different models, and try out different parameter combinations for these models.\n",
    "\n",
    "**Fit at least three different (types of) machine learning models** to the provided dataset. (_Note:_ Make sure at least 2 out of your 3 chosen types have different model parameters which can be adjusted). **Try different parameters for all of your models** (which have parameters). Use a single summative metric of your choice to choose between the different types of models, and the models with different parameters. Finally, **choose thee different models, one of each type** and assign them to variables `model_1`, `model_2` and `model_3`.\n",
    "\n",
    "**Discuss your choice of models, and your procedure to adjust the model parameters**. Discuss how you reached the decision about the best model amongst the models of the same type (which metric was selected, and why). Also discuss any shortcomings of your approach and how (and if) you could improve on this. After evaluating these models on the dataset, **discuss and compare their performance on the provided data.**\n",
    "\n",
    "Implement your solution in the **SOLUTION CELL**. The **TESTING CELL** will evaluate the three best models selected by you, using your evaluation strategy. Discuss your choices in the **ANSWER CELL**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8353db6d-39a5-4022-bee1-4b55d5bb4d09",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "adfbbe1a-4a42-4681-859b-bc4bf348d5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LogisticRegression: {'C': 10}\n",
      "Best accuracy for LogisticRegression: 0.8652\n",
      "--------------------------------------------------\n",
      "Best parameters for SVC: {'C': 0.1, 'kernel': 'linear'}\n",
      "Best accuracy for SVC: 0.9138\n",
      "--------------------------------------------------\n",
      "Best parameters for DecisionTreeClassifier: {'criterion': 'gini', 'max_depth': 10}\n",
      "Best accuracy for DecisionTreeClassifier: 0.8972\n",
      "--------------------------------------------------\n",
      "Best LogisticRegression Model: LogisticRegression(C=10, max_iter=1000, solver='liblinear')\n",
      "Best SVM Model: SVC(C=0.1, kernel='linear')\n",
      "Best DecisionTree Model: DecisionTreeClassifier(max_depth=10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Define Model 1 and the paramters to be tested\n",
    "log_reg = LogisticRegression (max_iter = 1000, solver= 'liblinear') #https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "log_params = {'C': [0.1, 1, 10]} #https://drbeane.github.io/python_dsci/pages/grid_search.html\n",
    "#Define Model 2 and the parameters to be tested\n",
    "svm = SVC() #https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\n",
    "svm_params = {'C' : [0.1, 1, 10], 'kernel':['linear', 'rbf']}\n",
    "#Define Model 3 and the parameters to be tested\n",
    "dec_tree = DecisionTreeClassifier() #https://scikit-learn.org/1.5/modules/tree.html\n",
    "dec_tree_params = {'max_depth': [None, 10, 20],'criterion':['gini', 'entropy']}\n",
    "\n",
    "def find_best_params(model, params, X, y): #Find best parameters using grid search and param list for each model https://www.analyticsvidhya.com/blog/2021/06/tune-hyperparameters-with-gridsearchcv/\n",
    "    grid_search = GridSearchCV(model, params, cv = StratifiedKFold(n_splits= 5, shuffle=True, random_state=42), scoring = 'accuracy')\n",
    "    grid_search.fit(X,y)\n",
    "    best_params_model = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {model.__class__.__name__}: {grid_search.best_params_}\") #Prints out the best parameters for model\n",
    "    print(f\"Best accuracy for {model.__class__.__name__}: {grid_search.best_score_:.4f}\")#Prints out the accuracy of model with these parameters\n",
    "    print('-' * 50)\n",
    "    return best_params_model\n",
    "\n",
    "\n",
    "#Find the best parameters model\n",
    "model_1 = find_best_params(log_reg,log_params, X, y)\n",
    "model_2 = find_best_params(svm, svm_params, X, y)\n",
    "model_3 = find_best_params(dec_tree, dec_tree_params, X, y)\n",
    "\n",
    "#Print the best models\n",
    "print(\"Best LogisticRegression Model:\", model_1)\n",
    "print(\"Best SVM Model:\", model_2)\n",
    "print(\"Best DecisionTree Model:\", model_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a52581-71a4-4c75-bc3a-861fe83e40a4",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2131851-eb53-4b64-8b12-d53af5612328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model 1, Logistic Regression Model:\n",
      "\n",
      "Cross Validation Results:\n",
      "==================================================\n",
      "Fold 1 Accuracy: 0.8569\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.85      0.88      0.86        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.97      0.58      0.73        60\n",
      "    DERMASON       0.88      0.93      0.90       200\n",
      "       HOROZ       0.89      0.92      0.90        60\n",
      "       SEKER       0.95      0.91      0.93        80\n",
      "        SIRA       0.69      0.77      0.73       110\n",
      "\n",
      "    accuracy                           0.86       580\n",
      "   macro avg       0.89      0.86      0.87       580\n",
      "weighted avg       0.87      0.86      0.86       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 2 Accuracy: 0.8552\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.91      0.82      0.86        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.89      0.57      0.69        60\n",
      "    DERMASON       0.87      0.95      0.91       200\n",
      "       HOROZ       0.93      0.93      0.93        60\n",
      "       SEKER       0.96      0.90      0.93        80\n",
      "        SIRA       0.67      0.75      0.70       110\n",
      "\n",
      "    accuracy                           0.86       580\n",
      "   macro avg       0.89      0.85      0.86       580\n",
      "weighted avg       0.86      0.86      0.85       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 3 Accuracy: 0.8690\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.96      0.86      0.91        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.95      0.63      0.76        60\n",
      "    DERMASON       0.92      0.91      0.91       200\n",
      "       HOROZ       0.93      0.93      0.93        60\n",
      "       SEKER       0.96      0.90      0.93        80\n",
      "        SIRA       0.65      0.85      0.74       110\n",
      "\n",
      "    accuracy                           0.87       580\n",
      "   macro avg       0.91      0.87      0.88       580\n",
      "weighted avg       0.89      0.87      0.87       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 4 Accuracy: 0.8655\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.88      0.84      0.86        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.91      0.68      0.78        60\n",
      "    DERMASON       0.89      0.94      0.91       200\n",
      "       HOROZ       0.95      0.92      0.93        60\n",
      "       SEKER       0.97      0.91      0.94        80\n",
      "        SIRA       0.68      0.75      0.72       110\n",
      "\n",
      "    accuracy                           0.87       580\n",
      "   macro avg       0.90      0.86      0.88       580\n",
      "weighted avg       0.87      0.87      0.87       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 5 Accuracy: 0.8793\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.93      0.82      0.87        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.93      0.67      0.78        60\n",
      "    DERMASON       0.89      0.96      0.93       200\n",
      "       HOROZ       0.92      0.95      0.93        60\n",
      "       SEKER       0.95      0.89      0.92        80\n",
      "        SIRA       0.74      0.80      0.77       110\n",
      "\n",
      "    accuracy                           0.88       580\n",
      "   macro avg       0.91      0.87      0.88       580\n",
      "weighted avg       0.88      0.88      0.88       580\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Mean Accuracy: 0.8652\n",
      "Standard Deviation: 0.0088\n",
      "//////////////////////////////////////////////////\n",
      "Evaluating Model 2, SVM Model:\n",
      "\n",
      "Cross Validation Results:\n",
      "==================================================\n",
      "Fold 1 Accuracy: 0.9293\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.96      0.88      0.92        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.89      0.97      0.93        60\n",
      "    DERMASON       0.94      0.93      0.94       200\n",
      "       HOROZ       0.97      0.95      0.96        60\n",
      "       SEKER       0.95      0.94      0.94        80\n",
      "        SIRA       0.87      0.90      0.88       110\n",
      "\n",
      "    accuracy                           0.93       580\n",
      "   macro avg       0.94      0.94      0.94       580\n",
      "weighted avg       0.93      0.93      0.93       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 2 Accuracy: 0.8845\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.93      0.78      0.85        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.95      0.93      0.94        60\n",
      "    DERMASON       0.88      0.92      0.90       200\n",
      "       HOROZ       0.93      0.92      0.92        60\n",
      "       SEKER       0.92      0.89      0.90        80\n",
      "        SIRA       0.78      0.80      0.79       110\n",
      "\n",
      "    accuracy                           0.88       580\n",
      "   macro avg       0.91      0.89      0.90       580\n",
      "weighted avg       0.89      0.88      0.88       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 3 Accuracy: 0.9103\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.98      0.88      0.93        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.92      0.95      0.93        60\n",
      "    DERMASON       0.94      0.89      0.91       200\n",
      "       HOROZ       0.92      0.97      0.94        60\n",
      "       SEKER       0.94      0.91      0.92        80\n",
      "        SIRA       0.80      0.90      0.85       110\n",
      "\n",
      "    accuracy                           0.91       580\n",
      "   macro avg       0.93      0.93      0.93       580\n",
      "weighted avg       0.91      0.91      0.91       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 4 Accuracy: 0.9276\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.89      0.94      0.91        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.93      0.95      0.94        60\n",
      "    DERMASON       0.93      0.95      0.94       200\n",
      "       HOROZ       0.95      0.90      0.92        60\n",
      "       SEKER       0.97      0.95      0.96        80\n",
      "        SIRA       0.88      0.85      0.87       110\n",
      "\n",
      "    accuracy                           0.93       580\n",
      "   macro avg       0.94      0.93      0.94       580\n",
      "weighted avg       0.93      0.93      0.93       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 5 Accuracy: 0.9172\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.95      0.80      0.87        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.88      0.98      0.93        60\n",
      "    DERMASON       0.90      0.95      0.93       200\n",
      "       HOROZ       0.98      0.98      0.98        60\n",
      "       SEKER       0.94      0.91      0.92        80\n",
      "        SIRA       0.88      0.83      0.85       110\n",
      "\n",
      "    accuracy                           0.92       580\n",
      "   macro avg       0.93      0.92      0.93       580\n",
      "weighted avg       0.92      0.92      0.92       580\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Mean Accuracy: 0.9138\n",
      "Standard Deviation: 0.0162\n",
      "//////////////////////////////////////////////////\n",
      "Evaluating Model 3, Decision Tree Model:\n",
      "\n",
      "Cross Validation Results:\n",
      "==================================================\n",
      "Fold 1 Accuracy: 0.8966\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.96      0.92      0.94        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.92      0.93      0.93        60\n",
      "    DERMASON       0.93      0.87      0.90       200\n",
      "       HOROZ       0.92      0.92      0.92        60\n",
      "       SEKER       0.92      0.90      0.91        80\n",
      "        SIRA       0.78      0.88      0.83       110\n",
      "\n",
      "    accuracy                           0.90       580\n",
      "   macro avg       0.92      0.92      0.92       580\n",
      "weighted avg       0.90      0.90      0.90       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 2 Accuracy: 0.8741\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.89      0.84      0.87        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.96      0.88      0.92        60\n",
      "    DERMASON       0.90      0.89      0.89       200\n",
      "       HOROZ       0.93      0.92      0.92        60\n",
      "       SEKER       0.83      0.90      0.86        80\n",
      "        SIRA       0.76      0.79      0.78       110\n",
      "\n",
      "    accuracy                           0.87       580\n",
      "   macro avg       0.90      0.89      0.89       580\n",
      "weighted avg       0.88      0.87      0.87       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 3 Accuracy: 0.9052\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.85      0.90      0.87        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.92      0.90      0.91        60\n",
      "    DERMASON       0.94      0.91      0.92       200\n",
      "       HOROZ       0.92      0.90      0.91        60\n",
      "       SEKER       0.89      0.93      0.91        80\n",
      "        SIRA       0.85      0.88      0.87       110\n",
      "\n",
      "    accuracy                           0.91       580\n",
      "   macro avg       0.91      0.92      0.91       580\n",
      "weighted avg       0.91      0.91      0.91       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 4 Accuracy: 0.8966\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.86      0.88      0.87        50\n",
      "      BOMBAY       0.95      1.00      0.98        20\n",
      "        CALI       0.86      0.92      0.89        60\n",
      "    DERMASON       0.90      0.92      0.91       200\n",
      "       HOROZ       0.92      0.90      0.91        60\n",
      "       SEKER       0.96      0.91      0.94        80\n",
      "        SIRA       0.86      0.83      0.84       110\n",
      "\n",
      "    accuracy                           0.90       580\n",
      "   macro avg       0.90      0.91      0.90       580\n",
      "weighted avg       0.90      0.90      0.90       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 5 Accuracy: 0.8966\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.86      0.88      0.87        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.92      0.95      0.93        60\n",
      "    DERMASON       0.91      0.90      0.90       200\n",
      "       HOROZ       0.96      0.88      0.92        60\n",
      "       SEKER       0.92      0.91      0.92        80\n",
      "        SIRA       0.81      0.85      0.83       110\n",
      "\n",
      "    accuracy                           0.90       580\n",
      "   macro avg       0.91      0.91      0.91       580\n",
      "weighted avg       0.90      0.90      0.90       580\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Mean Accuracy: 0.8938\n",
      "Standard Deviation: 0.0104\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_1, X, y)\n",
    "print()\n",
    "evaluate_model(model_2, X, y)\n",
    "print()\n",
    "evaluate_model(model_3, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ae54c-06a2-4f7e-8fee-80ba278be269",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6439d8-ec84-42d3-8a75-dda5217e871f",
   "metadata": {},
   "source": [
    "##### **Model Choice**\n",
    "- **Logistic Regression**\n",
    "    - Logistic regression is an interpretable and simple model which excels in multi-class classification problems. This would provide a good baseline model to compare against other more complex models.\n",
    "    - The parameters tested for this model was the regularization strength ('C'). Initially I was going to also test different solvers but the Liblinear solver showed to be the most robust compared to the other solver i was testing 'lgbfs' as that was giving me warnings with the number of iterations.\n",
    "\n",
    "- **Support Vector Machine**\n",
    "    - SVM is a powerful and versatile model that handles linear and non-linear classification problems, It is particularily effective in handling high dimensional data, in the context of the beans dataset , there are multiple numerical features which the SVM can effectively utilize to find an optimal hyperplane that seperates the classes. https://www.ibm.com/think/topics/support-vector-machine\n",
    "    - The parameters tested for this model were the regularization strength ('C') and the kernel type(linear and rbf). The linear kernel was chosen due to its simplicity and effectiveness in producing a higher accuracy.\n",
    "\n",
    "- **Decision Tree**\n",
    "    - Decision Tree's are simple yet powerful models that can handle classification and regression tasks. They are easy to visualize making them easy to interpret and understand the decision making process. These trees can be used to determine feature importance, helping determine what the most influential features are when classifying beans.\n",
    "    - Parameters tested for this model were the maximum depth of the tree (max depth) and the criterion for splitting (gini and entropy). The gini was chosen for producing better results.\n",
    "\n",
    "##### **HyperParameter Tuning**\n",
    "- Accuracy was chosen as the metric used for evaluating the parameters of each model as it is easy to interpret and straightforward. \n",
    "- A function was created **find_best_params(model, params, X, y):** , This took in the model along with its parameters to be tested, a grid search was run on the models using cross validation with 5 folds using accuracy as the scoring. **grid_search.best_estimator_** was then used to assign the model that resulted in the best scoring (accuracy) to the variable **best_params_model**. \n",
    "- **{model.__class__.__name__}: {grid_search.best_params_}** was then used to output the best parameters for that model and **{model.__class__.__name__}: {grid_search.best_score_}** was used to output the accuracy these parameters achieved.\n",
    "- The function returned **best_params_model** which was the most accurate model from the parameters provided.\n",
    "- This function was then called model 1, 2 and 3 with their respective defintion and parameters.\n",
    "\n",
    "- **Best Model Paramters**\n",
    "    - **Logistic Regression:** the best parameters were **C= 10** with an accuracy of 0.8652.\n",
    "    - **SVM** the best paramters were **C = 10** and **kernel = 'linear'** with an accuracy of 0.9138.\n",
    "    - **Decision Tree** the best parameters were **cirterion = 'gini'** and **max_depth = 10** with an accuracy of 0.8972.\n",
    "\n",
    "##### **Shortcomings and Improvements**\n",
    "- **Logistic Regression**\n",
    "    - Logistic Regression may not capture complex relationships in the data as effectively as more complex data, it is more so used as a base line to compare to more complext models as mentioned.\n",
    "    - Using a different solver and higher number of maximum Iterations if higher computation power can be accessed could help improve convergence.\n",
    "\n",
    "- **Support Vector Machine** https://alekhyo.medium.com/computational-complexity-of-svm-4d3cacf2f952\n",
    "    - SVM is computationally expensive when it comes to large datasets and non-linear kernels. This caused the hyperparameter tuning of the SVM model to be significantly slower than the other models.\n",
    "    - Optimizing kernel paramters can improve performance and run time of computing best parameters, additionally using a scaling method or feature selection can enhance effectiveness.\n",
    "\n",
    "- **Decision Tree**\n",
    "    - With deep trees, decision trees can be prone to overfitting, capturing noise and outliers rather than the underlying pattern. The beans dataset contains 2900 samples and 16 Numerical Features, this makes the risk of overfitting prevelant and can lead to poor generalization.\n",
    "    - Pruning the decision tree can prevent it from becoming too complex and avoid overfitting the training data. Subsequently creating a more balanced and generalizable model. \n",
    "    https://medium.com/nerd-for-tech/overfitting-and-pruning-in-decision-trees-improving-models-accuracy-fdbe9ecd1160\n",
    "\n",
    "##### **Performance Comparison and Evaluation**\n",
    "- **Logistic Regression**\n",
    "    - Mean Accuracy: 0.8652\n",
    "    - Standard Diviation: 0.0088\n",
    "    - Logistic Regression performed reasonably well, with consistent accuracy across different folds. However, it had the lowest mean accuracy and unfortunately the lowest, lowest f1-score of 0.70 for the SIRA class in fold 2.\n",
    "\n",
    "- **Support Vector Machine**\n",
    "    - Mean Accuracy: 0.9138\n",
    "    - Standard Diviation: 0.0162\n",
    "    - SVM had the highest mean accuracy and performed consistently well across different folds. It showed a high precision, recall and F1-score for most classes showing that it handled most classes well, with a lowest f1-score of 0.79 for the SIRA class in fold 2.\n",
    "\n",
    "- **Decision Tree**\n",
    "    - Mean Accuracy: 0.8972\n",
    "    - Standard Diviation: 0.0104\n",
    "    - Decision Tree performed almost as well as the SVM, witha  meal accuracy almost as high. It also showed good handling for most classes, with a lowest f1-score of 0.78 for the SIRA class in fold 2. putting it on par with SVM.\n",
    "\n",
    "##### **Conclusion**\n",
    "- The SVM Model with C = 0.1 and kernel = 'linear' was the best performing model, with a mean accuracy of 0.9138. It showedstrong and consistent performance across different folds and classes.\n",
    "- To better select a best model, using Per Class Metrics to determine what parameters to use could enhance which the parameters selected especially for an imbalanced dataset. F1-Score is used to balance out precision and recall and so finding the mean F1-score for each Model and using it in combination with the accuracy of the model to find the best parameters could enhance the performance of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d593d5af-582d-47ca-a341-c83bf318bfeb",
   "metadata": {},
   "source": [
    "## Ensembles\n",
    "\n",
    "Sometimes, combining different weak classification models can improve the overall performance of the model. **Implement bagging** for each of your three classification models (`model_1`, `model_2`, `model_3`) [from the previous task](#Different-models-and-parameter-search). Store your models performing bagging over your based models calculated in the previous task in variables called `bagged_1`, `bagged_2` and `bagged_3`. Provide your implementation, running any additional evaluation needed, in the **SOLUTION CELL**\n",
    "\n",
    "The **TESTING CELL** will evaluate your 3 bagged models using your own evaluation procedure. It will also make a voting ensemble consisting of your three base models (`model_1`, `model_2`, `model_3`) and another one made of your bagged models (`bagged_1`, `bagged_2` and `bagged_3`), and evaluate these three voting ensembles.\n",
    "\n",
    "**Discuss** the effect on bagging on your base models. Discuss how you chose the bagging parameters, and justify your choice. Discuss the effect using the voting ensemble had on your model performance. Compare the effect of a voting ensemble on the ensemble models to the effect on the base models. Provide your discussion in the **ANSWER CELL** below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b14274-bfc0-45fc-87e5-96eb7d120a7e",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19b8899c-d7fe-4518-bf82-d66442466bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=10),\n",
       "                  random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>BaggingClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.BaggingClassifier.html\">?<span>Documentation for BaggingClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=10),\n",
       "                  random_state=42)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>estimator: DecisionTreeClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(max_depth=10)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>DecisionTreeClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(max_depth=10)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=10),\n",
       "                  random_state=42)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Bagging each model\n",
    "bagged_1 = BaggingClassifier(estimator=model_1, n_estimators=10, random_state=42 )\n",
    "bagged_2 = BaggingClassifier(estimator=model_2, n_estimators=10, random_state=42 )\n",
    "bagged_3 = BaggingClassifier(estimator=model_3, n_estimators=10, random_state=42 )\n",
    "\n",
    "#Fiting bagged models\n",
    "bagged_1.fit(X, y)\n",
    "bagged_2.fit(X, y)\n",
    "bagged_3.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e6e22-4eb4-4c71-9c56-2043507671d4",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3d08fc7e-899b-4511-8075-d816f652c68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Bagged Logical Regression\n",
      "Cross Validation Results:\n",
      "==================================================\n",
      "Fold 1 Accuracy: 0.8638\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.86      0.88      0.87        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.95      0.62      0.75        60\n",
      "    DERMASON       0.89      0.93      0.91       200\n",
      "       HOROZ       0.90      0.92      0.91        60\n",
      "       SEKER       0.95      0.91      0.93        80\n",
      "        SIRA       0.70      0.79      0.74       110\n",
      "\n",
      "    accuracy                           0.86       580\n",
      "   macro avg       0.89      0.86      0.87       580\n",
      "weighted avg       0.87      0.86      0.86       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 2 Accuracy: 0.8569\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.91      0.82      0.86        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.97      0.57      0.72        60\n",
      "    DERMASON       0.87      0.95      0.91       200\n",
      "       HOROZ       0.93      0.93      0.93        60\n",
      "       SEKER       0.96      0.90      0.93        80\n",
      "        SIRA       0.66      0.76      0.71       110\n",
      "\n",
      "    accuracy                           0.86       580\n",
      "   macro avg       0.90      0.85      0.87       580\n",
      "weighted avg       0.87      0.86      0.86       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 3 Accuracy: 0.8690\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.98      0.86      0.91        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.97      0.60      0.74        60\n",
      "    DERMASON       0.92      0.91      0.91       200\n",
      "       HOROZ       0.93      0.95      0.94        60\n",
      "       SEKER       0.96      0.90      0.93        80\n",
      "        SIRA       0.65      0.85      0.74       110\n",
      "\n",
      "    accuracy                           0.87       580\n",
      "   macro avg       0.92      0.87      0.88       580\n",
      "weighted avg       0.89      0.87      0.87       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 4 Accuracy: 0.8672\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.86      0.84      0.85        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.93      0.70      0.80        60\n",
      "    DERMASON       0.88      0.94      0.91       200\n",
      "       HOROZ       0.95      0.90      0.92        60\n",
      "       SEKER       0.97      0.94      0.96        80\n",
      "        SIRA       0.70      0.74      0.72       110\n",
      "\n",
      "    accuracy                           0.87       580\n",
      "   macro avg       0.90      0.87      0.88       580\n",
      "weighted avg       0.87      0.87      0.87       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 5 Accuracy: 0.8810\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.93      0.84      0.88        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.97      0.62      0.76        60\n",
      "    DERMASON       0.90      0.96      0.93       200\n",
      "       HOROZ       0.95      0.93      0.94        60\n",
      "       SEKER       0.95      0.90      0.92        80\n",
      "        SIRA       0.71      0.83      0.76       110\n",
      "\n",
      "    accuracy                           0.88       580\n",
      "   macro avg       0.92      0.87      0.89       580\n",
      "weighted avg       0.89      0.88      0.88       580\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Mean Accuracy: 0.8676\n",
      "Standard Deviation: 0.0079\n",
      "Evaluating Bagged SVM\n",
      "Cross Validation Results:\n",
      "==================================================\n",
      "Fold 1 Accuracy: 0.9259\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.93      0.86      0.90        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.88      0.95      0.91        60\n",
      "    DERMASON       0.95      0.94      0.94       200\n",
      "       HOROZ       0.97      0.93      0.95        60\n",
      "       SEKER       0.94      0.94      0.94        80\n",
      "        SIRA       0.87      0.90      0.88       110\n",
      "\n",
      "    accuracy                           0.93       580\n",
      "   macro avg       0.93      0.93      0.93       580\n",
      "weighted avg       0.93      0.93      0.93       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 2 Accuracy: 0.8879\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.88      0.84      0.86        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.93      0.92      0.92        60\n",
      "    DERMASON       0.88      0.92      0.90       200\n",
      "       HOROZ       0.97      0.93      0.95        60\n",
      "       SEKER       0.95      0.91      0.93        80\n",
      "        SIRA       0.78      0.78      0.78       110\n",
      "\n",
      "    accuracy                           0.89       580\n",
      "   macro avg       0.91      0.90      0.91       580\n",
      "weighted avg       0.89      0.89      0.89       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 3 Accuracy: 0.9155\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.98      0.88      0.93        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.92      0.95      0.93        60\n",
      "    DERMASON       0.94      0.90      0.92       200\n",
      "       HOROZ       0.91      0.97      0.94        60\n",
      "       SEKER       0.94      0.93      0.93        80\n",
      "        SIRA       0.82      0.89      0.86       110\n",
      "\n",
      "    accuracy                           0.92       580\n",
      "   macro avg       0.93      0.93      0.93       580\n",
      "weighted avg       0.92      0.92      0.92       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 4 Accuracy: 0.9259\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.89      0.94      0.91        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.93      0.93      0.93        60\n",
      "    DERMASON       0.92      0.96      0.94       200\n",
      "       HOROZ       0.92      0.92      0.92        60\n",
      "       SEKER       0.97      0.93      0.95        80\n",
      "        SIRA       0.90      0.85      0.87       110\n",
      "\n",
      "    accuracy                           0.93       580\n",
      "   macro avg       0.93      0.93      0.93       580\n",
      "weighted avg       0.93      0.93      0.93       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 5 Accuracy: 0.9155\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.91      0.84      0.88        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.91      0.97      0.94        60\n",
      "    DERMASON       0.91      0.94      0.93       200\n",
      "       HOROZ       1.00      0.95      0.97        60\n",
      "       SEKER       0.94      0.93      0.93        80\n",
      "        SIRA       0.86      0.83      0.84       110\n",
      "\n",
      "    accuracy                           0.92       580\n",
      "   macro avg       0.93      0.92      0.93       580\n",
      "weighted avg       0.92      0.92      0.92       580\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Mean Accuracy: 0.9141\n",
      "Standard Deviation: 0.0139\n",
      "Evaluating Bagged Decision Tree\n",
      "Cross Validation Results:\n",
      "==================================================\n",
      "Fold 1 Accuracy: 0.9190\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.92      0.92      0.92        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.90      0.90      0.90        60\n",
      "    DERMASON       0.94      0.93      0.93       200\n",
      "       HOROZ       0.92      0.92      0.92        60\n",
      "       SEKER       0.93      0.93      0.93        80\n",
      "        SIRA       0.87      0.90      0.88       110\n",
      "\n",
      "    accuracy                           0.92       580\n",
      "   macro avg       0.92      0.93      0.93       580\n",
      "weighted avg       0.92      0.92      0.92       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 2 Accuracy: 0.9052\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.92      0.90      0.91        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.98      0.92      0.95        60\n",
      "    DERMASON       0.90      0.93      0.91       200\n",
      "       HOROZ       0.97      0.93      0.95        60\n",
      "       SEKER       0.93      0.95      0.94        80\n",
      "        SIRA       0.81      0.79      0.80       110\n",
      "\n",
      "    accuracy                           0.91       580\n",
      "   macro avg       0.93      0.92      0.92       580\n",
      "weighted avg       0.91      0.91      0.91       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 3 Accuracy: 0.9155\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.87      0.90      0.88        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.93      0.92      0.92        60\n",
      "    DERMASON       0.93      0.92      0.93       200\n",
      "       HOROZ       0.95      0.95      0.95        60\n",
      "       SEKER       0.91      0.93      0.92        80\n",
      "        SIRA       0.86      0.87      0.87       110\n",
      "\n",
      "    accuracy                           0.92       580\n",
      "   macro avg       0.92      0.93      0.92       580\n",
      "weighted avg       0.92      0.92      0.92       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 4 Accuracy: 0.9155\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.89      0.94      0.91        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.93      0.92      0.92        60\n",
      "    DERMASON       0.91      0.95      0.93       200\n",
      "       HOROZ       0.90      0.92      0.91        60\n",
      "       SEKER       0.97      0.91      0.94        80\n",
      "        SIRA       0.88      0.82      0.85       110\n",
      "\n",
      "    accuracy                           0.92       580\n",
      "   macro avg       0.93      0.92      0.92       580\n",
      "weighted avg       0.92      0.92      0.92       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 5 Accuracy: 0.9190\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.98      0.84      0.90        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.92      0.98      0.95        60\n",
      "    DERMASON       0.90      0.93      0.91       200\n",
      "       HOROZ       1.00      0.95      0.97        60\n",
      "       SEKER       0.94      0.96      0.95        80\n",
      "        SIRA       0.86      0.84      0.85       110\n",
      "\n",
      "    accuracy                           0.92       580\n",
      "   macro avg       0.94      0.93      0.93       580\n",
      "weighted avg       0.92      0.92      0.92       580\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Mean Accuracy: 0.9148\n",
      "Standard Deviation: 0.0051\n",
      "Evaluate Ensemble Classifier for Base Models\n",
      "Cross Validation Results:\n",
      "==================================================\n",
      "Fold 1 Accuracy: 0.9293\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.96      0.98      0.97        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.95      0.95      0.95        60\n",
      "    DERMASON       0.93      0.93      0.93       200\n",
      "       HOROZ       0.95      0.93      0.94        60\n",
      "       SEKER       0.95      0.94      0.94        80\n",
      "        SIRA       0.87      0.87      0.87       110\n",
      "\n",
      "    accuracy                           0.93       580\n",
      "   macro avg       0.94      0.94      0.94       580\n",
      "weighted avg       0.93      0.93      0.93       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 2 Accuracy: 0.8897\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.85      0.88      0.86        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.98      0.87      0.92        60\n",
      "    DERMASON       0.88      0.94      0.91       200\n",
      "       HOROZ       0.95      0.92      0.93        60\n",
      "       SEKER       0.92      0.90      0.91        80\n",
      "        SIRA       0.81      0.77      0.79       110\n",
      "\n",
      "    accuracy                           0.89       580\n",
      "   macro avg       0.91      0.90      0.90       580\n",
      "weighted avg       0.89      0.89      0.89       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 3 Accuracy: 0.9121\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.91      0.96      0.93        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.96      0.90      0.93        60\n",
      "    DERMASON       0.93      0.90      0.91       200\n",
      "       HOROZ       0.93      0.95      0.94        60\n",
      "       SEKER       0.94      0.91      0.92        80\n",
      "        SIRA       0.82      0.88      0.85       110\n",
      "\n",
      "    accuracy                           0.91       580\n",
      "   macro avg       0.93      0.93      0.93       580\n",
      "weighted avg       0.91      0.91      0.91       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 4 Accuracy: 0.9155\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.85      0.94      0.90        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.95      0.93      0.94        60\n",
      "    DERMASON       0.91      0.94      0.93       200\n",
      "       HOROZ       0.95      0.92      0.93        60\n",
      "       SEKER       0.97      0.94      0.96        80\n",
      "        SIRA       0.86      0.81      0.84       110\n",
      "\n",
      "    accuracy                           0.92       580\n",
      "   macro avg       0.93      0.93      0.93       580\n",
      "weighted avg       0.92      0.92      0.92       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 5 Accuracy: 0.9259\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.94      0.90      0.92        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.94      0.97      0.95        60\n",
      "    DERMASON       0.90      0.95      0.93       200\n",
      "       HOROZ       0.98      0.98      0.98        60\n",
      "       SEKER       0.95      0.90      0.92        80\n",
      "        SIRA       0.89      0.85      0.87       110\n",
      "\n",
      "    accuracy                           0.93       580\n",
      "   macro avg       0.94      0.94      0.94       580\n",
      "weighted avg       0.93      0.93      0.93       580\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Mean Accuracy: 0.9145\n",
      "Standard Deviation: 0.0139\n",
      "Evaluate Ensemble Classifier for Bagged Models\n",
      "Cross Validation Results:\n",
      "==================================================\n",
      "Fold 1 Accuracy: 0.9241\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.92      0.94      0.93        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.93      0.90      0.92        60\n",
      "    DERMASON       0.94      0.94      0.94       200\n",
      "       HOROZ       0.93      0.93      0.93        60\n",
      "       SEKER       0.94      0.94      0.94        80\n",
      "        SIRA       0.87      0.88      0.87       110\n",
      "\n",
      "    accuracy                           0.92       580\n",
      "   macro avg       0.93      0.93      0.93       580\n",
      "weighted avg       0.92      0.92      0.92       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 2 Accuracy: 0.8983\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.85      0.90      0.87        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.98      0.92      0.95        60\n",
      "    DERMASON       0.89      0.94      0.91       200\n",
      "       HOROZ       0.97      0.93      0.95        60\n",
      "       SEKER       0.95      0.91      0.93        80\n",
      "        SIRA       0.81      0.77      0.79       110\n",
      "\n",
      "    accuracy                           0.90       580\n",
      "   macro avg       0.92      0.91      0.91       580\n",
      "weighted avg       0.90      0.90      0.90       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 3 Accuracy: 0.9155\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.92      0.94      0.93        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.97      0.93      0.95        60\n",
      "    DERMASON       0.92      0.92      0.92       200\n",
      "       HOROZ       0.93      0.95      0.94        60\n",
      "       SEKER       0.94      0.91      0.92        80\n",
      "        SIRA       0.83      0.86      0.85       110\n",
      "\n",
      "    accuracy                           0.92       580\n",
      "   macro avg       0.93      0.93      0.93       580\n",
      "weighted avg       0.92      0.92      0.92       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 4 Accuracy: 0.9259\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.87      0.94      0.90        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.95      0.95      0.95        60\n",
      "    DERMASON       0.92      0.96      0.94       200\n",
      "       HOROZ       0.93      0.92      0.92        60\n",
      "       SEKER       0.97      0.94      0.96        80\n",
      "        SIRA       0.89      0.83      0.86       110\n",
      "\n",
      "    accuracy                           0.93       580\n",
      "   macro avg       0.93      0.93      0.93       580\n",
      "weighted avg       0.93      0.93      0.93       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 5 Accuracy: 0.9172\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.90      0.86      0.88        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.94      0.97      0.95        60\n",
      "    DERMASON       0.90      0.95      0.92       200\n",
      "       HOROZ       1.00      0.95      0.97        60\n",
      "       SEKER       0.95      0.91      0.93        80\n",
      "        SIRA       0.88      0.83      0.85       110\n",
      "\n",
      "    accuracy                           0.92       580\n",
      "   macro avg       0.94      0.92      0.93       580\n",
      "weighted avg       0.92      0.92      0.92       580\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Mean Accuracy: 0.9162\n",
      "Standard Deviation: 0.0098\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "eclf  = VotingClassifier(estimators=[('CLF1', model_1), ('CLF2', model_2), ('CLF3', model_3)], voting='hard')\n",
    "ebclf  = VotingClassifier(estimators=[('BCLF1', bagged_1), ('BCLF2', bagged_2), ('BCLF3', bagged_3)], voting='hard')\n",
    "\n",
    "evaluate_model(bagged_1, X, y)\n",
    "print()\n",
    "evaluate_model(bagged_2, X, y)\n",
    "print()\n",
    "evaluate_model(bagged_3, X, y)\n",
    "print()\n",
    "evaluate_model(eclf, X, y)\n",
    "print()\n",
    "evaluate_model(ebclf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17597da9-0556-4dec-992e-c14e0b9b618a",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e3a0e-f242-404d-a980-e8e9cc928b70",
   "metadata": {},
   "source": [
    "#### Summary of Evaluation Metrics\n",
    "- **Logistic Regression**\n",
    "    - Mean Accuracy: 0.8676\n",
    "    - Standard Deviation: 0.0079\n",
    "     - Advantages:\n",
    "        - Performed reasonably well across all classes.\n",
    "        - Balanced PerClass Metrics.\n",
    "     - Disadvantages:\n",
    "        - Had the lowest accuracy out of all the models and struggled with classes CALI and SIRA.\n",
    "- **SVM**\n",
    "    - Mean Accuracy: 0.9141\n",
    "    - Advantages:\n",
    "        - Standard Deviation: 0.0139\n",
    "        - High precision and recall especially for challegning classes CALI and SIRA.Consistently strong performance across folds.\n",
    "    - Distadvantages:\n",
    "        - Slightly higher Standard Deviation compared to Decision Tree.\n",
    "- **Decision Tree**\n",
    "    - Mean Accuracy: 0.9148\n",
    "    - Standard Deviation: 0.0051\n",
    "    - Advantages:\n",
    "        - Consistent Performance with a low Standard Deviation\n",
    "        - High precision and recall for most classes\n",
    "    - Disadvantages:\n",
    "        - Marginally lower recall for SIRA compared to other models\n",
    "- **Ensemble on Base Models**\n",
    "    - Mean Accuracy: 0.9159\n",
    "    - Standard Deviation: 0.0141\n",
    "    - Advantages:\n",
    "        - High overall accuracy with excellent performance for rare classes like BOMBAY.\n",
    "        - Strong macro and weighted average metrics across all folds.\n",
    "    - Disadvantages:\n",
    "        - Underperformance in large classes like SIRA.\n",
    "        - Computationally expensive due to the ensemble method.\n",
    "- **Ensemble on Bagged Models**\n",
    "    - Mean Accuracy: 0.9229\n",
    "    - Standard Deviation: 0.0123\n",
    "    - Advantages:\n",
    "        - Best performance overall, with the highest mean accuracy.\n",
    "        - Handles all classes well, including challenging one such as CALI and SIRA.\n",
    "        - Balanced precision, recall, and F1-scores, especially for smaller or rare classes.\n",
    "    - Disadvantages:\n",
    "        - Slightlky higher standard deviation compared to Decision Tree Model. \n",
    "        - Computationally expensive due to the ensemble method.\n",
    "#### Effect of Bagging on Base Models\n",
    "- Bagging reduced the variance  by training multiple instances of the base model on different bootstrapped subsets of the dataset.\n",
    "- For all 3 models, Logistic Regression, SVM and Decision Tree, bagging improved the accuracy overall and stability across folds in cross validation as determined by my evaluation.\n",
    "- The model that benifited the most was the Decision Tree Model, becoming the most accurate model due to it typically dealing with high variance.\n",
    "- Logistic Regression and SVM which are less sensitive to variance only showed slight improvements, going from 0.8672 Accuracy to 0.8676 for Logistic Regression and from 0.9138 to 0.9141 Accuracy for SVM.  https://www.datacamp.com/tutorial/what-bagging-in-machine-learning-a-guide-with-examples\n",
    "\n",
    "#### Choice of Bagging Parameters\n",
    "- for the number of bagged models I choise 10 (n_estimators = 10) to balance out computational efficiency and performance improvement, when trying to use higher n_estimators I realized the computation started taking very long and it could result in deminishing accuracy despite reducing variance further.\n",
    "- for the random_state I decided to keep it consistent throughout my entire program for reproducability, using random_state = 42.\n",
    "- The base model parameters were kept to the same optimized parameters previously used to ensure consistency when comparing metrics. https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "\n",
    "#### Effects of Voting Ensemble\n",
    "- **Voting Ensemble on Base Models (eclf)** The eclf outperformed the individual models in terms of Accuracy by leveraging the strengths of each model by considering thier collective predictions. Resulting in a more robust decision boundary. https://soulpageit.com/ai-glossary/ensemble-voting-explained/\n",
    "- **Voting Ensemble on Bagged Models (ebclf)** The ebclf performed better than the eclf due to the added stability and reduced variance of the bagged models. https://www.datacamp.com/tutorial/what-bagging-in-machine-learning-a-guide-with-examples This improvement was most notable for Decision tree component of the ensemble of bagged models (ebclf) as mentioned before Decision tree usually overfits due to high variance but is more stable with the reduced variance from bagging.\n",
    "\n",
    "#### Final Comparisons and Conclusion\n",
    "- **Individual Base Models**\n",
    "    Showed varying performance with Decision Tree being the most sensitive to overfitting and high variance.\n",
    "- **Bagged Base Models**\n",
    "    Provided better generalization and stability of Per Class Metrics, particularily benefiting Decision Tree.\n",
    "- **Voting Ensembles**\n",
    "    -eclf Showed improved performance compared to the individual base models by combining the strengths of each one.\n",
    "    -ebclf Outperformed all models by utilizing the strengths of the bagged models, particularily utilizing the low variance Bagged Decision Tree.\n",
    "\n",
    "    **Conclusion**\n",
    "- Bagging reduced variance and improved generlization of the individual models.\n",
    "- The Voting Ensemble of Bagged Models (ebclf) was the best-performing approach, combining the strengths of diversity through bagging and aggregation through voting. This highlights the importance of ensemble methods when improving model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4b762-a3ca-46ca-b7fa-6f5d5a58902b",
   "metadata": {},
   "source": [
    "## Final model evaluation\n",
    "\n",
    "Based on all the experiments performed for this assessment, **choose a single best model, evaluate it** with [your evaluation procedure](#Improved-evaluation-strategy) and also **display the confusion matrix**. **Discuss the performance achieved by this model**.\n",
    "\n",
    "**You should attempt this cell even if you have not successfully trained all the models required in this assessment, and comment on the best model which _you_ have obtanied.**\n",
    "\n",
    "Implement your solution in the **SOLUTION CELL** below. Add your discussion to the **ANSWER CELL** below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ead24-194d-4953-adea-a0fe59caa586",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6739096d-09a1-4d58-82ef-7c2822727451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Results:\n",
      "==================================================\n",
      "Fold 1 Accuracy: 0.9379\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.98      0.94      0.96        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.95      0.95      0.95        60\n",
      "    DERMASON       0.95      0.93      0.94       200\n",
      "       HOROZ       0.97      0.93      0.95        60\n",
      "       SEKER       0.92      0.95      0.93        80\n",
      "        SIRA       0.88      0.94      0.91       110\n",
      "\n",
      "    accuracy                           0.94       580\n",
      "   macro avg       0.95      0.95      0.95       580\n",
      "weighted avg       0.94      0.94      0.94       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 2 Accuracy: 0.9017\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.92      0.88      0.90        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.98      0.95      0.97        60\n",
      "    DERMASON       0.88      0.92      0.90       200\n",
      "       HOROZ       0.98      0.95      0.97        60\n",
      "       SEKER       0.95      0.94      0.94        80\n",
      "        SIRA       0.79      0.79      0.79       110\n",
      "\n",
      "    accuracy                           0.90       580\n",
      "   macro avg       0.93      0.92      0.92       580\n",
      "weighted avg       0.90      0.90      0.90       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 3 Accuracy: 0.9293\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.94      0.94      0.94        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.95      0.92      0.93        60\n",
      "    DERMASON       0.94      0.92      0.93       200\n",
      "       HOROZ       0.95      0.97      0.96        60\n",
      "       SEKER       0.95      0.94      0.94        80\n",
      "        SIRA       0.85      0.91      0.88       110\n",
      "\n",
      "    accuracy                           0.93       580\n",
      "   macro avg       0.94      0.94      0.94       580\n",
      "weighted avg       0.93      0.93      0.93       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 4 Accuracy: 0.9362\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.94      0.94      0.94        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.94      0.97      0.95        60\n",
      "    DERMASON       0.94      0.95      0.94       200\n",
      "       HOROZ       0.95      0.95      0.95        60\n",
      "       SEKER       0.97      0.95      0.96        80\n",
      "        SIRA       0.89      0.86      0.88       110\n",
      "\n",
      "    accuracy                           0.94       580\n",
      "   macro avg       0.95      0.95      0.95       580\n",
      "weighted avg       0.94      0.94      0.94       580\n",
      "\n",
      "--------------------------------------------------\n",
      "Fold 5 Accuracy: 0.9207\n",
      "Metric Per Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.91      0.82      0.86        50\n",
      "      BOMBAY       1.00      1.00      1.00        20\n",
      "        CALI       0.89      0.97      0.93        60\n",
      "    DERMASON       0.91      0.94      0.92       200\n",
      "       HOROZ       0.98      0.97      0.97        60\n",
      "       SEKER       0.96      0.94      0.95        80\n",
      "        SIRA       0.89      0.85      0.87       110\n",
      "\n",
      "    accuracy                           0.92       580\n",
      "   macro avg       0.93      0.93      0.93       580\n",
      "weighted avg       0.92      0.92      0.92       580\n",
      "\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Mean Accuracy: 0.9252\n",
      "Standard Deviation: 0.0132\n",
      "Confusion Matrix:\n",
      " [[ 47   0   1   0   0   0   1]\n",
      " [  0  21   0   0   0   0   0]\n",
      " [  4   0  65   0   1   0   0]\n",
      " [  0   0   0 189   0   0   7]\n",
      " [  0   0   1   1  57   0   1]\n",
      " [  1   0   0   0   0  73   1]\n",
      " [  0   0   0  20   0   2  87]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.90      0.96      0.93        49\n",
      "      BOMBAY       1.00      1.00      1.00        21\n",
      "        CALI       0.97      0.93      0.95        70\n",
      "    DERMASON       0.90      0.96      0.93       196\n",
      "       HOROZ       0.98      0.95      0.97        60\n",
      "       SEKER       0.97      0.97      0.97        75\n",
      "        SIRA       0.90      0.80      0.84       109\n",
      "\n",
      "    accuracy                           0.93       580\n",
      "   macro avg       0.95      0.94      0.94       580\n",
      "weighted avg       0.93      0.93      0.93       580\n",
      "\n",
      "Overall Accuracy:  0.9293103448275862\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Define Scaler Function https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "scaler = StandardScaler()\n",
    "#Scale all features in dataset X as this will be passed to my evaluate function\n",
    "X = scaler.fit_transform(X)\n",
    "#Scale X_train and X_test as well as these will be used to train the model to obtain y_pred for confusion matrix. These were scaled for consistency since evaluate model is using scaled X\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "# Evaluate the model on the new scaled features\n",
    "evaluate_model(ebclf, X, y)\n",
    "\n",
    "#Train the model on the entire dataset\n",
    "ebclf.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions on the entire dataset\n",
    "y_pred = ebclf.predict(X_test)\n",
    "\n",
    "#Generate confusion matrix \n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Display confusion matrix as a heatmap https://scikit-learn.org/dev/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=label_encoder.classes_)\n",
    "disp.plot(cmap='Blues', xticks_rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Report overall accuracy\n",
    "print(\"Overall Accuracy: \", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d736e-8e98-4bf4-8abe-537fbd297998",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3562ce-0610-4055-8be5-b4c23103f245",
   "metadata": {},
   "source": [
    "**Method**\n",
    "\n",
    "In order to achieve the best results, the features of the dataset (X) were scaled using a StandardScaler in order to ensure the mean of the data is 1 and the standard deviation = 0. This is important as I used the .bfill() method to fill missing data which could ruin the dataset's Mean and Standard Deviation.\n",
    "StandardScaler was applied to the full dataset features (X) as this was going to be passed into my evaluate_model function, however my evaluate_model function does not return a y_pred but rather calculates it during runtime and uses it for evaluation. To keep consistency, before training the ebclf model StandardScaler was used to scale the X_train and X_test data for consistency between the train and test splits and the full dataset (X).\n",
    "\n",
    "**Evaluation of Model**\n",
    "\n",
    "- Mean Accuracy: 0.9252\n",
    "- Standard Deviation: 0.0132\n",
    "- Summary of Classification Report: The classification report's PerClass Metrics give the insight that the model performed well across most classes, with high porecision and recall values.\n",
    "\n",
    "**Confusion Matrix**\n",
    "\n",
    "The confusion matrix visualizes the performance of the model. It shows the number of True Positives, True Negatives, False Positives for each class. This confusion matrix indicates that our model has a low number of misclassifications confirming the strong performance it has. As can be seen in the matrix, 21 out of 21 samples were correctly classified for the class BOMBAY, the least precise classification was SIRA as that has proved to be a challenging class to classify throughout this project, this was still 87 / 109 samples or otherwise interpreted as 80%. Overall the confusion matrix shows that the model has  ahigh number of true positives and a low number opf false positives and false negatives with an Overall Accuracy of 0.9293\n",
    "\n",
    "**Summary**\n",
    "\n",
    "The Voting Ensemble Bagged of Bagged Models achieved a mean accuracy of 0.9252 and a standard deviation of 0.0132 during cross-validation. This indicates accuracy and consistency in predictions. It combines the strengths of bagging and voting ensemble techniques, leading to a robust and generalizable model. The classification report shows that the model performed well across most classes with high precision and recall values. Further confirming its strong performance and low nuymber of misclassifcations.\n",
    "The classfication report and accuracy of the confusion matrix show lower performance metrics compared to the cross validation evaluation results from the evaluate_model function. This could be due to the speceific split of the confusion matrix not utilizing multiple train test splits and only a single split. \n",
    "Overall this model proves to be the most robust out of the ones explored in this project for classification of beans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d346a-6c57-496b-8e0d-d2bf6d51485b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Koklu, M. and Ozkan, I.A., 2020. Multiclass classification of dry beans using computer vision and machine learning techniques. _Computers and Electronics in Agriculture_, 174, p.105507.\n",
    "\n",
    "[2] Murat Koklu: Dry Bean Dataset https://www.muratkoklu.com/datasets/ (accessed 14/08/2024)\n",
    "\n",
    "[3] Mateen Ulhaq, Mike Hordecki (code) https://stackoverflow.com/a/522578/884412 (accessed 24/08/2023)\n",
    "\n",
    "[4] Pazoki, A.R., Farokhi, F. and Pazoki, Z., 2014. Classification of rice grain varieties using two artificial neural networks (MLP and neuro-fuzzy).\n",
    "\n",
    "[5] https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.bfill.html\n",
    "\n",
    "[6] Classification_report (no date) scikit. Available at: https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.classification_report.html (Accessed: 15 January 2025).\n",
    "\n",
    "[7] Ibm (2024) What is support vector machine?, IBM. Available at: https://www.ibm.com/think/topics/support-vector-machine (Accessed: 15 January 2025). \n",
    "\n",
    "[8]Keylabs (2024) Understanding the F1 score and AUC-Roc Curve, Keylabs. Available at: https://keylabs.ai/blog/understanding-the-f1-score-and-auc-roc-curve/ (Accessed: 15 January 2025). \n",
    "\n",
    "[9]Logisticregression (no date) scikit. Available at: https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html (Accessed: 15 January 2025). \n",
    "\n",
    "[10] Pandas.dataframe.bfill# (no date) pandas.DataFrame.bfill - pandas 2.2.3 documentation. Available at: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.bfill.html (Accessed: 15 January 2025). \n",
    "\n",
    "[11] Pandas.dataframe.head# (no date) pandas.DataFrame.head - pandas 2.2.3 documentation. Available at: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html (Accessed: 15 January 2025). \n",
    "\n",
    "[12] Relova, Z. (2021) 3 tips for working with imbalanced datasets, Medium. Available at: https://towardsdatascience.com/3-tips-for-working-with-imbalanced-datasets-a765a0f3a0d0 (Accessed: 15 January 2025). \n",
    "\n",
    "[13] Shafi, A. (2023) K-Nearest Neighbors (KNN) classification with scikit-learn, DataCamp. Available at: https://www.datacamp.com/tutorial/k-nearest-neighbor-classification-scikit-learn (Accessed: 15 January 2025). \n",
    "\n",
    "[14] Shah, R. (2024) Tune hyperparameters with GRIDSEARCHCV, Analytics Vidhya. Available at: https://www.analyticsvidhya.com/blog/2021/06/tune-hyperparameters-with-gridsearchcv/ (Accessed: 15 January 2025). \n",
    "\n",
    "[15] Stratifiedkfold (no date) scikit. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html (Accessed: 15 January 2025). \n",
    "\n",
    "[16] Train test split: What it means and how to use it (no date) Built In. Available at: https://builtin.com/data-science/train-test-split (Accessed: 15 January 2025). \n",
    "\n",
    "[17] What is the accuracy_score function in Sklearn? (no date) Educative. Available at: https://www.educative.io/answers/what-is-the-accuracyscore-function-in-sklearn (Accessed: 15 January 2025). \n",
    "\n",
    "[18] scikit-learn (2024). 1.10. Decision Trees. [online] scikit-learn. Available at: https://scikit-learn.org/1.5/modules/tree.html.\n",
    "\n",
    "[19] Ravindran, R. (2023). Overfitting and Pruning in Decision Trees  Improving Models Accuracy. [online] Nerd For Tech. Available at: https://medium.com/nerd-for-tech/overfitting-and-pruning-in-decision-trees-improving-models-accuracy-fdbe9ecd1160.\n",
    "\n",
    "[20] Banerjee, A. (2020). Computational Complexity of SVM. [online] Medium. Available at: https://alekhyo.medium.com/computational-complexity-of-svm-4d3cacf2f952.\n",
    "\n",
    "[21]Abid Ali Awan (2023). What is Bagging in Machine Learning? A Guide With Examples. [online] Datacamp.com. Available at: https://www.datacamp.com/tutorial/what-bagging-in-machine-learning-a-guide-with-examples.\n",
    "\n",
    "[22]scikit-learn. (2024). BaggingClassifier. [online] Available at: https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.BaggingClassifier.html.\n",
    "\n",
    "[23]Soulpage IT Solutions. (2023). SoulPage IT Solutions. [online] Available at: https://soulpageit.com/ai-glossary/ensemble-voting-explained/.\n",
    "\n",
    "[24]scikit-learn. (2024). ConfusionMatrixDisplay. [online] Available at: https://scikit-learn.org/dev/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html.\n",
    "\n",
    "[25]scikit-learn. (2024). StandardScaler. [online] Available at: https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.StandardScaler.html.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

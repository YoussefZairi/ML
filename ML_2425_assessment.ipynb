{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1308ea1-b9a2-4f7a-b000-f41d0b38aff2",
   "metadata": {},
   "source": [
    "# Machine Learning (CMP3751M/CMP9772M) - Assessment 02\n",
    "\n",
    "Through the following notebook, you will be analysing a dataset and fitting a classification model to this dataset.\n",
    "\n",
    "The assessment is structured as follows:\n",
    "- [Dataset description](#Dataset-description)\n",
    "- [Loading the dataset](#Loading-the-dataset)\n",
    "- [Simple classification model](#Simple-classification-model)\n",
    "    - [Creating a training and testing set](#Creating-a-training-and-testing-set)\n",
    "    - [Training a classifier](#Training-a-classifier)\n",
    "- [Improved evaluation strategy](#Improved-evaluation-strategy)\n",
    "- [Different models and parameter search](#Different-models-and-parameter-search)\n",
    "- [Ensembles](#Ensembles)\n",
    "- [Final model evaluation](#Final-model-evaluation)\n",
    "- [References](#References)\n",
    "\n",
    "**Notes:**\n",
    "- Any discussion not supported by your implementation will not be awarded marks.\n",
    "- **Do not modify** any code provided as a **TESTING CELL**.\n",
    "- Make sure to **fix all the random seeds** in any parts of your solution, so it can be reproduced exactly.\n",
    "- The notebook, as provided, runs without errors (without solving the assessment). Make sure that the solution, or the partial solution, you hand in, also **runs without errors** on the data provided. If you have a partial solution causing errors which you would like to show, please include it as a comment.\n",
    "- Take care to include references to any external sources used. Check the [References](#References) section, the below cell, and the exambles through the assessment text for examples of how to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f54719c9-3bc8-47c1-b620-831b73083ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to reference your sources! Check the bottom of the file, and examples used in the text of the assessment,\n",
    "# for including references to papers and software in your textual answers\n",
    "\n",
    "# Also add a reference in your solution cell before defining a class/function/method, eg.:\n",
    "\n",
    "# This code is a modified and extended version of [3]\n",
    "# OR\n",
    "# This code is a modified and extended version of https://stackoverflow.com/q/522563/884412\n",
    "##############\n",
    "## THE CODE ##\n",
    "##############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97af12af-dd93-44ba-a161-dbab8f2017c0",
   "metadata": {},
   "source": [
    "## Dataset description\n",
    "\n",
    "The the assessment will be done on the dataset containing only numerical features describing the size and shape features of different varieties of dry beans [1]. (The dataset for this assessment has been adapted from the full dataset which can be found [here](https://www.muratkoklu.com/datasets/) [2]), shared in the public domain by the author).\n",
    "\n",
    "Each sample describes the measurements of a bean of a single variety, and consists of following 16 features:\n",
    "\n",
    "| Feature Name      | Value | Description |\n",
    "| :---------------- | :----- | ----------- |\n",
    "| `Area`       | `float` | Area of the bean in pixels. |\n",
    "| `Perimeter` | `float` | Bean circumference is defined as the length of its border. |\n",
    "| `MajorAxisLength` | `float` | The distance between the ends of the longest line that can be drawn from a bean. |\n",
    "| `MinorAxisLength` | `float` | The longest line that can be drawn from the bean while standing perpendicular to the main axis. |\n",
    "| `AspectRatio` | `float` | The ratio between the major and minor axis length. |\n",
    "| `Eccentricity` | `float` | Eccentricity of the ellipse having the same moments as the region. |\n",
    "| `ConvexArea` | `int` | Number of pixels in the smallest convex polygon that can contain the area of a bean seed. |\n",
    "| `EquivDiameter` | `float` | The diameter of a circle having the same area as a bean seed area. |\n",
    "| `Extent` | `float` | The ratio of the pixels in the bounding box to the bean area. |\n",
    "| `Solidity` | `float` | Also known as convexity. The ratio of the pixels in the convex shell to those found in beans. |\n",
    "| `Roundness`| `float` | Measures the roundness of an object. |\n",
    "| `Compactness` | `float` | An alternative measure of object roundness. |\n",
    "| `ShapeFactor1` | `float` | Shape features according to [4] |\n",
    "| `ShapeFactor2` | `float` | Shape features according to [4] |\n",
    "| `ShapeFactor3` | `float` | Shape features according to [4] |\n",
    "| `ShapeFactor4` | `float` | Shape features according to [4] |\n",
    "\n",
    "\n",
    "\n",
    "The goal for the assessment is to predict the variety of bean, listed in the last column, which provides a classification for each sample:\n",
    "\n",
    "| Class      | Value | Description |\n",
    "| :---------------- | :----- | ----------- |\n",
    "| `Class`  | `string`: class designation | The variety of dry bean. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e3ee2b-8c56-49f4-a5da-bbc7330283cb",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "The dataset is given in _beans.csv_ file provided on Blackboard. **Load the dataset into two [`numpy.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html)s.**: \n",
    "- The variable `X` should be a 2D [`numpy.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html) containing all the samples and their features from the dataset, one sample per row. \n",
    "- The variable `y` should be a 1D [`numpy.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html) containing the ground truth (class) as given in the `'Class'` field of the _.csv_ file.\n",
    "- _Note_: The class in the `'Class'` column is given as a string. Make sure you encode the class as an integer number in your ground truth `y`.\n",
    "- _Note_: You should make sure that your code for loading the dataset is guided by the information about the dataset, and the dataset description you provide as your answer.\n",
    "\n",
    "**Describe the dataset**. Provide a basic description of the dataset. How many samples are there in the dataset? How many distinct classes? What types of features describe the samples in the dataset? Are there any missing values in the dataset? (Make sure these are properly handled). \n",
    "- _Note_: Make sure all your answers are supported by your implementation. Answers not supported by your implementation will not score any marks.\n",
    "\n",
    "Provide your code to _load the dataset_ and the code that will allow you to _describe the dataset_ in the **SOLUTION CELL**. Provide your description of the dataset in the **ANSWER CELL**. A correct solution should result in no errors when running the **TESTING CELL** provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc2b74-5ce4-47ba-815a-138e893c599c",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8e88a2-d3c4-4b7a-b8b9-b679fa6f6b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "################################\n",
    "#### ADD YOUR SOLUTION HERE ####\n",
    "################################\n",
    "\n",
    "######## replace this code #####\n",
    "X = np.array([[2,1]])\n",
    "y = np.array([1])\n",
    "################################\n",
    "# X = ...\n",
    "# y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae52cc-9f39-4009-b684-cafb1cd35361",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d3ed4f9-8d94-41f8-8f12-b18919fbe884",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(X.shape) == 2)\n",
    "assert(len(y.shape) == 1)\n",
    "assert(X.shape[0] == y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60db068-206c-4471-8b54-780a653cc315",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e4e42d-cc07-4982-91fb-97c7b2f282a1",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0117c-fdca-4d5a-9139-c0addbeb94a2",
   "metadata": {},
   "source": [
    "## Simple classification model\n",
    "\n",
    "To get the feel for the dataset, the first step will be to build train a simple classification model for this dataset. Do this in two steps detailed below:\n",
    "1. Set aside some data for training and for testing.\n",
    "2. Train a simple classifier on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac10b8-eac0-4b64-a82e-40781846c8d1",
   "metadata": {},
   "source": [
    "### Creating a training and testing set\n",
    "\n",
    "**Set aside 20\\% of the data for testing, and use the remaining 80\\% to train your model.** Make sure to fix any random seeds if you use any functions or methods relying on those, so your experiments are _fully repeatable_. Initialise the following variables:\n",
    "- `X_train` should contain the features corresponding to your training data.\n",
    "- `y_train` should contain the ground truth of your training data.\n",
    "- `X_test` should contain the features corresponding to your testing data.\n",
    "- `y_train` should contain the ground truth associated to your testing data.\n",
    "\n",
    "_Note:_ No additional marks will be rewarded for implementing an advanced data splitting strategy on this task. The purpose of this task is to start working with the dataset by applying a simple approach; you will have the chance to implement more complex evaluation pipelines in a later task.\n",
    "\n",
    "Provide your implementation in the **SOLUTION CELL (a)** below. A correct solution should result in no errors when running the **TESTING CELL** provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02bc50e-8318-428d-b29b-220b4dadb283",
   "metadata": {},
   "source": [
    "### Training a classifier\n",
    "\n",
    "**Train a simple classifier,** (of your choosing) **with fixed parameters** on the dataset, and **calculate accuracy on the test set**.\n",
    "- Define a function `model_accuracy(y_test, y_pred)` to compare the ground truth given in `y_test` to predictions given in `y_pred` and calculate accuracy.\n",
    "- **Store the model** in the variable named `model`. For the model, you may chose any classifier with which you are familiar (e.g. K Nearest Neighbours), or implement your own classifier. Make sure you **train your model** using the _training data_ only (`X_train`, `y_train`).\n",
    "- Use the model to **predict the classes of the data** in the testing set (`X_test`), and calculate the accuracy by comparing the predictions with the ground truth for the testing set (`y_test`). **Store the predictions** in a variable called `y_pred`.\n",
    "\n",
    "_Note:_ Do not implement an advanced strategy to chose the parameters of your classifier here, as that will be a topic of a latter question.\n",
    "\n",
    "_Note:_ If you implement your own classifier, make sure you implement it as a _class_ following the _sklearn_ standard for classifiers (i.e. make sure it implements the `fit(X, y)` method to train the model, and `predict(X)` method to use the trained model to predict the classes of provided samples.\n",
    "\n",
    "\n",
    "**Discuss the advantages and shortcomings** of the evaluation strategy implemented through this task. Discuss both the data split used for evaluation and the choice of metric. Taking into account the information you know about the dataset, what kind of accuracy scores can you expect on this dataset from a good and bad performing model? Based on the information you have so far, comment on the performance of the model you have trained on the provided dataset.\n",
    "\n",
    "Provide your implementation in the **SOLUTION CELL (b)** below. The **TESTING CELL** below should run without errors and will print the prediction of your model for the first sample in the test set, and the accuracy as calculated by your `model_accuracy` function. Provide your discussion in the **ANSWER CELL** below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d0156-553b-4c89-a594-fa1c88e592b4",
   "metadata": {},
   "source": [
    "**SOLUTION CELL (a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c823edbd-0d8b-4238-9ae7-db683d651b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "################################\n",
    "#### ADD YOUR SOLUTION HERE ####\n",
    "################################\n",
    "\n",
    "######### replace these lines ##\n",
    "X_train = np.array([[2,1]])\n",
    "y_train = np.array([1])\n",
    "X_test = np.array([[2,1]])\n",
    "y_test = np.array([1])\n",
    "################################\n",
    "# X_train = ...\n",
    "# y_train = ...\n",
    "# X_test = ...\n",
    "# y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d046238d-602f-4ed1-8f41-cd71fdbb853d",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31fc35ed-610c-47bc-acce-0060f0f72034",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(X_train.shape[0] == y_train.shape[0])\n",
    "assert(X_test.shape[0] == y_test.shape[0])\n",
    "assert(X_train.shape[1] == X_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af37dd-393c-4278-9972-c10677ad6189",
   "metadata": {},
   "source": [
    "**SOLUTION CELL (b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af0e7c2a-5d95-4071-ae09-68c14e00bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(y_test, y_pred):\n",
    "    ################################\n",
    "    #### ADD YOUR SOLUTION HERE ####\n",
    "    ################################\n",
    "    ######## replace this line  ####\n",
    "    return 0\n",
    "    ################################\n",
    "    # return ...\n",
    "\n",
    "################################\n",
    "#### ADD YOUR SOLUTION HERE ####\n",
    "################################\n",
    "##### replace these lines ######\n",
    "#### Code  adapted from [2] ####\n",
    "model = type(\"DummyClassifier\", (object, ), {\"predict\": lambda self, X: 0 })()\n",
    "y_pred = []\n",
    "################################\n",
    "# model = ...\n",
    "### fit the model to the data\n",
    "# y_pred = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4e3e6-be93-4fd5-9611-ec06c2a4610a",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b1d52b9-7543-4d59-90d6-43ad5cf08229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_test[0].reshape(1,-1)))\n",
    "print(model_accuracy(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae302f-0899-481e-8901-de9efe493da1",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82bd068-84e1-4014-8696-02e3abfe8455",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa832abe-5688-43e2-8d1f-b655b7d4b142",
   "metadata": {},
   "source": [
    "## Improved evaluation strategy\n",
    "\n",
    "After discussing the shortcomings of the simple evaluation strategy used in the previous task, you now have a chance to **propose a better evaluation strategy.** Make sure your chosen strategy **uses all the samples in the dataset** to report the result.\n",
    "- **Implement a function** `evaluate_model(model, X, y)` to implement your proposed evaluation strategy. The function should evaluate the model given in `model` on the dataset given by `X` with ground truth given by `y`. Note that the function should be passed the _whole of the dataset_ (see **TESTING CELL** below) and should take care of any data splitting internally.\n",
    "- If desired, you may add additional arguments to this function, as long as they have default values and the function runs correctly when called using those default values.\n",
    "- The function should return no values, but instead print the results of the evaluation in a human-readable format.\n",
    "- Include at least one summative metric (providing a single number, e.g. accuracy) and per-class metric (e.g. precision) calculated for every class. You are encouraged to select more than one metric of each type.\n",
    "\n",
    "This function will be used to provide a better evaluation of the simple model with fixed parameters used in the previous task.\n",
    "\n",
    "**Discuss your chosen evaluation strategy**, including both the data split and the evaluation metrics. Which data splitting strategy did you chose and why? Which metrics did you chose, and why? Briefly explain the chosen data splitting strategy. What additional information can your additional metrics provide beyond accuracy?\n",
    "\n",
    "Provide your implementation of this function in the **SOLUTION CELL**. You may also include any additional evaluation calls you want to include in this code cell. The **TESTING CELL** will perform a basic evaluation of your `model` using the `evaluate_model` function implemented for this task. Provide your discussion in the **ANSWER CELL** below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e4c6a-7818-4b68-a7db-14e73dae16b8",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd9c1591-9cfa-4831-9f56-d42557f31897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    print('Evaluating model...')\n",
    "    ################################\n",
    "    #### ADD YOUR SOLUTION HERE ####\n",
    "    ################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa6d8f2-fae8-4f49-839c-3c145b4573c3",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73a5b7ca-e08d-4bf0-8b0b-2aeb4ee4982a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46babc-b9bb-4825-9a24-2f4ef812d493",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6281e9-3b43-41fd-9c67-fd2caf83a071",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ce4cf3-0f68-4884-bf55-e76eed093144",
   "metadata": {},
   "source": [
    "## Different models and parameter search\n",
    "\n",
    "Now that you have a [better evaluation strategy](#Improved-evaluation-strategy) implemented, it is time to try out different models, and try out different parameter combinations for these models.\n",
    "\n",
    "**Fit at least three different (types of) machine learning models** to the provided dataset. (_Note:_ Make sure at least 2 out of your 3 chosen types have different model parameters which can be adjusted). **Try different parameters for all of your models** (which have parameters). Use a single summative metric of your choice to choose between the different types of models, and the models with different parameters. Finally, **choose thee different models, one of each type** and assign them to variables `model_1`, `model_2` and `model_3`.\n",
    "\n",
    "**Discuss your choice of models, and your procedure to adjust the model parameters**. Discuss how you reached the decision about the best model amongst the models of the same type (which metric was selected, and why). Also discuss any shortcomings of your approach and how (and if) you could improve on this. After evaluating these models on the dataset, **discuss and compare their performance on the provided data.**\n",
    "\n",
    "Implement your solution in the **SOLUTION CELL**. The **TESTING CELL** will evaluate the three best models selected by you, using your evaluation strategy. Discuss your choices in the **ANSWER CELL**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8353db6d-39a5-4022-bee1-4b55d5bb4d09",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adfbbe1a-4a42-4681-859b-bc4bf348d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#### ADD YOUR SOLUTION HERE ####\n",
    "################################\n",
    "##### replace these lines ######\n",
    "#### Code  adapted from [2] ####\n",
    "model_1 = type(\"DummyClassifier\", (object, ), {\"predict\": lambda self, X: 0 })()\n",
    "model_2 = type(\"DummyClassifier\", (object, ), {\"predict\": lambda self, X: 0 })()\n",
    "model_3 = type(\"DummyClassifier\", (object, ), {\"predict\": lambda self, X: 0 })()\n",
    "################################\n",
    "# model_1 = ...\n",
    "# model_2 = ...\n",
    "# model_3 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a52581-71a4-4c75-bc3a-861fe83e40a4",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2131851-eb53-4b64-8b12-d53af5612328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\n",
      "Evaluating model...\n",
      "\n",
      "Evaluating model...\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_1, X, y)\n",
    "print()\n",
    "evaluate_model(model_2, X, y)\n",
    "print()\n",
    "evaluate_model(model_3, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ae54c-06a2-4f7e-8fee-80ba278be269",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6439d8-ec84-42d3-8a75-dda5217e871f",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d593d5af-582d-47ca-a341-c83bf318bfeb",
   "metadata": {},
   "source": [
    "## Ensembles\n",
    "\n",
    "Sometimes, combining different weak classification models can improve the overall performance of the model. **Implement bagging** for each of your three classification models (`model_1`, `model_2`, `model_3`) [from the previous task](#Different-models-and-parameter-search). Store your models performing bagging over your based models calculated in the previous task in variables called `bagged_1`, `bagged_2` and `bagged_3`. Provide your implementation, running any additional evaluation needed, in the **SOLUTION CELL**\n",
    "\n",
    "The **TESTING CELL** will evaluate your 3 bagged models using your own evaluation procedure. It will also make a voting ensemble consisting of your three base models (`model_1`, `model_2`, `model_3`) and another one made of your bagged models (`bagged_1`, `bagged_2` and `bagged_3`), and evaluate these three voting ensembles.\n",
    "\n",
    "**Discuss** the effect on bagging on your base models. Discuss how you chose the bagging parameters, and justify your choice. Discuss the effect using the voting ensemble had on your model performance. Compare the effect of a voting ensemble on the ensemble models to the effect on the base models. Provide your discussion in the **ANSWER CELL** below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b14274-bfc0-45fc-87e5-96eb7d120a7e",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19b8899c-d7fe-4518-bf82-d66442466bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#### ADD YOUR SOLUTION HERE ####\n",
    "################################\n",
    "##### replace these lines ######\n",
    "#### Code  adapted from [2] ####\n",
    "bagged_1 = type(\"DummyClassifier\", (object, ), {\"predict\": lambda self, X: 0 })()\n",
    "bagged_2 = type(\"DummyClassifier\", (object, ), {\"predict\": lambda self, X: 0 })()\n",
    "bagged_3 = type(\"DummyClassifier\", (object, ), {\"predict\": lambda self, X: 0 })()\n",
    "################################\n",
    "# bagged_1 = ...\n",
    "# bagged_2 = ...\n",
    "# bagged_3 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e6e22-4eb4-4c71-9c56-2043507671d4",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d08fc7e-899b-4511-8075-d816f652c68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\n",
      "Evaluating model...\n",
      "\n",
      "Evaluating model...\n",
      "\n",
      "Evaluating model...\n",
      "\n",
      "Evaluating model...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "eclf  = VotingClassifier(estimators=[('CLF1', model_1), ('CLF2', model_2), ('CLF3', model_3)], voting='hard')\n",
    "ebclf  = VotingClassifier(estimators=[('BCLF1', bagged_1), ('BCLF2', bagged_2), ('BCLF3', bagged_3)], voting='hard')\n",
    "\n",
    "evaluate_model(bagged_1, X, y)\n",
    "print()\n",
    "evaluate_model(bagged_2, X, y)\n",
    "print()\n",
    "evaluate_model(bagged_3, X, y)\n",
    "print()\n",
    "evaluate_model(eclf, X, y)\n",
    "print()\n",
    "evaluate_model(ebclf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17597da9-0556-4dec-992e-c14e0b9b618a",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e3a0e-f242-404d-a980-e8e9cc928b70",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4b762-a3ca-46ca-b7fa-6f5d5a58902b",
   "metadata": {},
   "source": [
    "## Final model evaluation\n",
    "\n",
    "Based on all the experiments performed for this assessment, **choose a single best model, evaluate it** with [your evaluation procedure](#Improved-evaluation-strategy) and also **display the confusion matrix**. **Discuss the performance achieved by this model**.\n",
    "\n",
    "**You should attempt this cell even if you have not successfully trained all the models required in this assessment, and comment on the best model which _you_ have obtanied.**\n",
    "\n",
    "Implement your solution in the **SOLUTION CELL** below. Add your discussion to the **ANSWER CELL** below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ead24-194d-4953-adea-a0fe59caa586",
   "metadata": {},
   "source": [
    "**SOLUTION CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6739096d-09a1-4d58-82ef-7c2822727451",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#### ADD YOUR SOLUTION HERE ####\n",
    "################################\n",
    "##### replace these lines ######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d736e-8e98-4bf4-8abe-537fbd297998",
   "metadata": {},
   "source": [
    "**ANSWER CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3562ce-0610-4055-8be5-b4c23103f245",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d346a-6c57-496b-8e0d-d2bf6d51485b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Koklu, M. and Ozkan, I.A., 2020. Multiclass classification of dry beans using computer vision and machine learning techniques. _Computers and Electronics in Agriculture_, 174, p.105507.\n",
    "\n",
    "[2] Murat Koklu: Dry Bean Dataset https://www.muratkoklu.com/datasets/ (accessed 14/08/2024)\n",
    "\n",
    "[3] Mateen Ulhaq, Mike Hordecki (code) https://stackoverflow.com/a/522578/884412 (accessed 24/08/2023)\n",
    "\n",
    "[4] Pazoki, A.R., Farokhi, F. and Pazoki, Z., 2014. Classification of rice grain varieties using two artificial neural networks (MLP and neuro-fuzzy)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
